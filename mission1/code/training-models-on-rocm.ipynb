{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQUk3Y0WwYZ4"
   },
   "source": [
    "# Train Models Using LeRobot on MI300x\n",
    "\n",
    "This guide walks you through setting up environment for training imitation learning policies using LeRobot library on a DigitalOcean (DO) instance equipped with AMD MI300x GPUs and ROCm.\n",
    "\n",
    "## ⚙️ Requirements\n",
    "- A Hugging Face dataset repo ID containing your training data (`--dataset.repo_id=${HF_USER}/${DATASET_NAME}`).\n",
    "  If you don’t have an access token yet, you can sign up for Hugging Face [here](https://huggingface.co/join). After signing up, create an access token by visiting [here](https://huggingface.co/settings/tokens).\n",
    "- A wandb account to enable training visualization and upload your training evidence to our github.\n",
    "  You can sign up for Wandb [here](https://wandb.ai/signup) and visit [here](https://wandb.ai/authorize) to create a token.\n",
    "- Access to DO instance AMD Mi300x GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJyX0CnwA5m"
   },
   "source": [
    "## Verify ROCm and GPU availability\n",
    "This cell uses `pytorch` to check AMD GPU Info. The expected ouput is \n",
    "```\n",
    "CUDA compatible device availability: True\n",
    "device name [0]: AMD Instinct MI300X VF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA compatible device availability: True\n",
      "device name [0]: AMD Instinct MI300X VF\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'CUDA compatible device availability:',torch.cuda.is_available())\n",
    "print(f'device name [0]:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJyX0CnwA5m"
   },
   "source": [
    "## Install FFmpeg 7.x\n",
    "This cell uses `apt` to install ffmpeg 7.x for LeRobot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlKjL1X5t_zM",
    "outputId": "3b375aa1-19ed-4811-ff76-0afd5b762992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: 'Types: deb\n",
      "URIs: https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu/\n",
      "Suites: noble\n",
      "Components: main\n",
      "'\n",
      "Description:\n",
      "unofficial build for FFmpeg 7 for Ubuntu 22.04 | 24.04, backport from Debian's deb.multimedia.org repository\n",
      "\n",
      "If the packages here are helpful, you may buy me a coffee:\n",
      "\n",
      "         https://ko-fi.com/ubuntuhandbook1\n",
      "More info: https://launchpad.net/~ubuntuhandbook1/+archive/ubuntu/ffmpeg7\n",
      "Adding repository.\n",
      "Hit:1 https://repo.radeon.com/amdgpu/30.10/ubuntu jammy InRelease\n",
      "Hit:2 https://repo.radeon.com/rocm/apt/7.0 jammy InRelease                     \n",
      "Get:3 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]      \n",
      "Hit:4 https://repo.radeon.com/graphics/7.0/ubuntu jammy InRelease              \n",
      "Hit:5 http://archive.ubuntu.com/ubuntu noble InRelease                         \n",
      "Get:6 http://security.ubuntu.com/ubuntu noble-security/main amd64 Packages [1700 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]      \n",
      "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease [17.8 kB]\n",
      "Get:9 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble InRelease [18.1 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu noble-security/main amd64 Components [25.3 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Packages [1182 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Components [90.5 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Packages [2796 kB]\n",
      "Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble/main amd64 Packages [40.2 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Components [156 B]\n",
      "Get:16 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Packages [33.1 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu noble-security/multiverse amd64 Components [157 B]\n",
      "Get:18 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 Packages [7835 B]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu noble-backports InRelease [126 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [2059 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 Components [235 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1943 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Components [520 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Packages [2929 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Components [158 B]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Packages [35.9 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu noble-updates/multiverse amd64 Components [888 B]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Packages [49.4 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu noble-backports/main amd64 Components [7885 B]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Packages [34.3 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu noble-backports/universe amd64 Components [11.9 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu noble-backports/restricted amd64 Components [161 B]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu noble-backports/multiverse amd64 Components [162 B]\n",
      "Fetched 14.1 MB in 2s (6906 kB/s)          \n",
      "Reading package lists... Done\n",
      "Hit:1 https://repo.radeon.com/amdgpu/30.10/ubuntu jammy InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu noble-security InRelease               \u001b[0m\u001b[33m\n",
      "Hit:3 https://repo.radeon.com/rocm/apt/7.0 jammy InRelease                     \u001b[0m\n",
      "Hit:4 https://repo.radeon.com/graphics/7.0/ubuntu jammy InRelease              \u001b[0m\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu noble InReleasem                        \u001b[0m\u001b[33m\n",
      "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu noble InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu noble-updates InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble InRelease\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu noble-backports InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "91 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  alsa-topology-conf alsa-ucm-conf libaribb24-0t64 libasound2-data\n",
      "  libasound2t64 libass9 libasyncns0 libavc1394-0 libavcodec-dev libavcodec61\n",
      "  libavdevice61 libavfilter10 libavformat-dev libavformat61 libavutil-dev\n",
      "  libavutil59 libbs2b0 libcaca0 libcdio-cdda2t64 libcdio-paranoia2t64\n",
      "  libcdio19t64 libdavs2-16 libdecor-0-0 libdecor-0-plugin-1-gtk libdvdnav4\n",
      "  libdvdread8t64 libfdk-aac2 libfftw3-double3 libflac12t64 libflite1\n",
      "  libiec61883-0 libilbc3 libjack-jackd2-0 libkvazaar7 libldb2 liblilv-0-0\n",
      "  liblmdb0 libmysofa1 libopenal-data libopenal1 libopencore-amrnb0\n",
      "  libopencore-amrwb0 libopenh264-7 libplacebo338 libpostproc58 libpulse0\n",
      "  librubberband2 libsamplerate0 libsdl2-2.0-0 libserd-0-0 libslang2\n",
      "  libsmbclient0 libsndfile1 libsndio7.0 libsord-0-0 libsratom-0-0\n",
      "  libswresample-dev libswresample5 libswscale-dev libswscale8 libtalloc2\n",
      "  libtdb1 libtevent0t64 libunibreak5 libvidstab1.1 libvmaf1 libvo-amrwbenc0\n",
      "  libvvenc1.12 libwbclient0 libxavs2-13 libxv1 libzimg2 libzix-0-0 samba-libs\n",
      "Suggested packages:\n",
      "  nvidia-vdpau-driver ffmpeg-doc alsa-utils libasound2-plugins\n",
      "  libnvidia-encode1 libdvdcss2 libfftw3-bin libfftw3-dev jackd2 libportaudio2\n",
      "  pulseaudio xdg-utils serdi sndiod sordi\n",
      "The following NEW packages will be installed:\n",
      "  alsa-topology-conf alsa-ucm-conf ffmpeg libaribb24-0t64 libasound2-data\n",
      "  libasound2t64 libass9 libasyncns0 libavc1394-0 libavcodec61 libavdevice61\n",
      "  libavfilter10 libavformat61 libavutil59 libbs2b0 libcaca0 libcdio-cdda2t64\n",
      "  libcdio-paranoia2t64 libcdio19t64 libdavs2-16 libdecor-0-0\n",
      "  libdecor-0-plugin-1-gtk libdvdnav4 libdvdread8t64 libfdk-aac2\n",
      "  libfftw3-double3 libflac12t64 libflite1 libiec61883-0 libilbc3\n",
      "  libjack-jackd2-0 libkvazaar7 libldb2 liblilv-0-0 liblmdb0 libmysofa1\n",
      "  libopenal-data libopenal1 libopencore-amrnb0 libopencore-amrwb0\n",
      "  libopenh264-7 libplacebo338 libpostproc58 libpulse0 librubberband2\n",
      "  libsamplerate0 libsdl2-2.0-0 libserd-0-0 libslang2 libsmbclient0 libsndfile1\n",
      "  libsndio7.0 libsord-0-0 libsratom-0-0 libswresample5 libswscale8 libtalloc2\n",
      "  libtdb1 libtevent0t64 libunibreak5 libvidstab1.1 libvmaf1 libvo-amrwbenc0\n",
      "  libvvenc1.12 libwbclient0 libxavs2-13 libxv1 libzimg2 libzix-0-0 samba-libs\n",
      "The following packages will be upgraded:\n",
      "  libavcodec-dev libavformat-dev libavutil-dev libswresample-dev\n",
      "  libswscale-dev\n",
      "5 upgraded, 70 newly installed, 0 to remove and 86 not upgraded.\n",
      "Need to get 55.3 MB of archives.\n",
      "After this operation, 124 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu noble/main amd64 libslang2 amd64 2.3.3-3build2 [506 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu noble/main amd64 liblmdb0 amd64 0.9.31-1build1 [48.1 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu noble/main amd64 alsa-topology-conf all 1.2.5.1-2 [15.5 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libasound2-data all 1.2.11-1ubuntu0.1 [21.1 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libasound2t64 amd64 1.2.11-1ubuntu0.1 [399 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 alsa-ucm-conf all 1.2.10-1ubuntu5.7 [66.4 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu noble/universe amd64 libaribb24-0t64 amd64 1.0.3-2.1build2 [31.0 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu noble/universe amd64 libdavs2-16 amd64 1.6-1build1 [259 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu noble/universe amd64 libfdk-aac2 amd64 2.0.2-3~ubuntu4 [332 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopencore-amrnb0 amd64 0.1.6-1build1 [98.2 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopencore-amrwb0 amd64 0.1.6-1build1 [53.2 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopenh264-7 amd64 2.4.1+dfsg-1 [409 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu noble/universe amd64 libvo-amrwbenc0 amd64 0.1.3-2build1 [76.7 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu noble/universe amd64 libxavs2-13 amd64 1.3-1 [546 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu noble/main amd64 libavc1394-0 amd64 0.5.4-5build3 [15.4 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu noble/universe amd64 libunibreak5 amd64 5.1-2build1 [25.0 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu noble/universe amd64 libass9 amd64 1:0.17.1-2build1 [104 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu noble/universe amd64 libdvdread8t64 amd64 6.1.3-1.1build1 [54.7 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu noble/universe amd64 libdvdnav4 amd64 6.1.1-3build1 [39.5 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu noble/main amd64 libtalloc2 amd64 2.4.2-1build2 [27.3 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu noble/main amd64 libtdb1 amd64 1.4.10-1build1 [46.8 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu noble/main amd64 libtevent0t64 amd64 0.16.1-2build1 [42.6 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libldb2 amd64 2:2.8.0+samba4.19.5+dfsg-4ubuntu9.4 [188 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libwbclient0 amd64 2:4.19.5+dfsg-4ubuntu9.4 [71.7 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 samba-libs amd64 2:4.19.5+dfsg-4ubuntu9.4 [6018 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libsmbclient0 amd64 2:4.19.5+dfsg-4ubuntu9.4 [62.4 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu noble/universe amd64 libbs2b0 amd64 3.1.0+dfsg-7build1 [10.6 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu noble/universe amd64 libflite1 amd64 2.2-6build3 [13.6 MB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu noble/universe amd64 libserd-0-0 amd64 0.32.2-1 [43.6 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu noble/universe amd64 libzix-0-0 amd64 0.4.2-2build1 [23.6 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu noble/universe amd64 libsord-0-0 amd64 0.16.16-2build1 [15.8 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu noble/universe amd64 libsratom-0-0 amd64 0.6.16-1build1 [17.3 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu noble/universe amd64 liblilv-0-0 amd64 0.24.22-1build1 [41.0 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu noble/universe amd64 libmysofa1 amd64 1.3.2+dfsg-2ubuntu2 [1158 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu noble/main amd64 libfftw3-double3 amd64 3.3.10-1ubuntu3 [838 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu noble/main amd64 libsamplerate0 amd64 0.2.2-4build1 [1344 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu noble/universe amd64 librubberband2 amd64 3.3.0+dfsg-2build1 [130 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu noble/universe amd64 libvidstab1.1 amd64 1.1.0-2build1 [38.5 kB]\n",
      "Get:39 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libavutil59 amd64 7:7.1.1-0build1~ubuntu2404 [367 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu noble/universe amd64 libzimg2 amd64 3.0.5+ds1-1build1 [254 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu noble/main amd64 libcaca0 amd64 0.99.beta20-4build2 [208 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libcdio19t64 amd64 2.1.0-4.1ubuntu1.2 [62.4 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu noble/main amd64 libcdio-cdda2t64 amd64 10.2+2.0.1-1.1build2 [16.5 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu noble/main amd64 libcdio-paranoia2t64 amd64 10.2+2.0.1-1.1build2 [16.6 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu noble/main amd64 libiec61883-0 amd64 1.2.0-6build1 [24.5 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu noble/main amd64 libjack-jackd2-0 amd64 1.9.21~dfsg-3ubuntu3 [289 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopenal-data all 1:1.23.1-4build1 [161 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu noble/universe amd64 libsndio7.0 amd64 1.9.0-0.3build3 [29.6 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopenal1 amd64 1:1.23.1-4build1 [540 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu noble/main amd64 libasyncns0 amd64 0.8-6build4 [11.3 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu noble/main amd64 libflac12t64 amd64 1.4.3+ds-2.1ubuntu2 [197 kB]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libsndfile1 amd64 1.2.2-1ubuntu5.24.04.1 [209 kB]\n",
      "Get:53 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpulse0 amd64 1:16.1+dfsg1-2ubuntu10.1 [292 kB]\n",
      "Get:54 http://archive.ubuntu.com/ubuntu noble/main amd64 libdecor-0-0 amd64 0.2.2-1build2 [16.5 kB]\n",
      "Get:55 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libsdl2-2.0-0 amd64 2.30.0+dfsg-1ubuntu3.1 [686 kB]\n",
      "Get:56 http://archive.ubuntu.com/ubuntu noble/main amd64 libxv1 amd64 2:1.0.11-1.1build1 [10.7 kB]\n",
      "Get:57 http://archive.ubuntu.com/ubuntu noble/universe amd64 libplacebo338 amd64 6.338.2-2build1 [2654 kB]\n",
      "Get:58 http://archive.ubuntu.com/ubuntu noble/main amd64 libdecor-0-plugin-1-gtk amd64 0.2.2-1build2 [22.2 kB]\n",
      "Get:59 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libilbc3 amd64 3.0.4-0build1~noble [52.6 kB]\n",
      "Get:60 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libkvazaar7 amd64 2.2.0-0build1~noble [259 kB]\n",
      "Get:61 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libswresample5 amd64 7:7.1.1-0build1~ubuntu2404 [74.6 kB]\n",
      "Get:62 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libvvenc1.12 amd64 1.12.0-0build5~ubuntu2404 [1219 kB]\n",
      "Get:63 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libavcodec61 amd64 7:7.1.1-0build1~ubuntu2404 [6093 kB]\n",
      "Get:64 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libavformat61 amd64 7:7.1.1-0build1~ubuntu2404 [1227 kB]\n",
      "Get:65 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libpostproc58 amd64 7:7.1.1-0build1~ubuntu2404 [61.5 kB]\n",
      "Get:66 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libswscale8 amd64 7:7.1.1-0build1~ubuntu2404 [218 kB]\n",
      "Get:67 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libvmaf1 amd64 2.3.1-0build2~noble [170 kB]\n",
      "Get:68 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libavfilter10 amd64 7:7.1.1-0build1~ubuntu2404 [1784 kB]\n",
      "Get:69 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libavdevice61 amd64 7:7.1.1-0build1~ubuntu2404 [94.1 kB]\n",
      "Get:70 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 ffmpeg amd64 7:7.1.1-0build1~ubuntu2404 [1984 kB]\n",
      "Get:71 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libswscale-dev amd64 7:7.1.1-0build1~ubuntu2404 [253 kB]\n",
      "Get:72 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libavformat-dev amd64 7:7.1.1-0build1~ubuntu2404 [1483 kB]\n",
      "Get:73 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libavcodec-dev amd64 7:7.1.1-0build1~ubuntu2404 [6805 kB]\n",
      "Get:74 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libswresample-dev amd64 7:7.1.1-0build1~ubuntu2404 [92.1 kB]\n",
      "Get:75 https://ppa.launchpadcontent.net/ubuntuhandbook1/ffmpeg7/ubuntu noble/main amd64 libavutil-dev amd64 7:7.1.1-0build1~ubuntu2404 [530 kB]\n",
      "Fetched 55.3 MB in 33s (1669 kB/s)                                             \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libslang2:amd64.\n",
      "(Reading database ... 98342 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libslang2_2.3.3-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libslang2:amd64 (2.3.3-3build2) ...\n",
      "Selecting previously unselected package liblmdb0:amd64.\n",
      "Preparing to unpack .../01-liblmdb0_0.9.31-1build1_amd64.deb ...\n",
      "Unpacking liblmdb0:amd64 (0.9.31-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  1%]\u001b[49m\u001b[39m [..........................................................] \u001b8Selecting previously unselected package alsa-topology-conf.\n",
      "Preparing to unpack .../02-alsa-topology-conf_1.2.5.1-2_all.deb ...\n",
      "Unpacking alsa-topology-conf (1.2.5.1-2) ...\n",
      "Selecting previously unselected package libasound2-data.\n",
      "Preparing to unpack .../03-libasound2-data_1.2.11-1ubuntu0.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libasound2-data (1.2.11-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libasound2t64:amd64.\n",
      "Preparing to unpack .../04-libasound2t64_1.2.11-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libasound2t64:amd64 (1.2.11-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Selecting previously unselected package alsa-ucm-conf.\n",
      "Preparing to unpack .../05-alsa-ucm-conf_1.2.10-1ubuntu5.7_all.deb ...\n",
      "Unpacking alsa-ucm-conf (1.2.10-1ubuntu5.7) ...\n",
      "Selecting previously unselected package libaribb24-0t64:amd64.\n",
      "Preparing to unpack .../06-libaribb24-0t64_1.0.3-2.1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libaribb24-0t64:amd64 (1.0.3-2.1build2) ...\n",
      "Selecting previously unselected package libavutil59:amd64.\n",
      "Preparing to unpack .../07-libavutil59_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking libavutil59:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Selecting previously unselected package libdavs2-16:amd64.\n",
      "Preparing to unpack .../08-libdavs2-16_1.6-1build1_amd64.deb ...\n",
      "Unpacking libdavs2-16:amd64 (1.6-1build1) ...\n",
      "Selecting previously unselected package libfdk-aac2:amd64.\n",
      "Preparing to unpack .../09-libfdk-aac2_2.0.2-3~ubuntu4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libfdk-aac2:amd64 (2.0.2-3~ubuntu4) ...\n",
      "Selecting previously unselected package libilbc3:amd64.\n",
      "Preparing to unpack .../10-libilbc3_3.0.4-0build1~noble_amd64.deb ...\n",
      "Unpacking libilbc3:amd64 (3.0.4-0build1~noble) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libkvazaar7:amd64.\n",
      "Preparing to unpack .../11-libkvazaar7_2.2.0-0build1~noble_amd64.deb ...\n",
      "Unpacking libkvazaar7:amd64 (2.2.0-0build1~noble) ...\n",
      "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
      "Preparing to unpack .../12-libopencore-amrnb0_0.1.6-1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking libopencore-amrnb0:amd64 (0.1.6-1build1) ...\n",
      "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
      "Preparing to unpack .../13-libopencore-amrwb0_0.1.6-1build1_amd64.deb ...\n",
      "Unpacking libopencore-amrwb0:amd64 (0.1.6-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Selecting previously unselected package libopenh264-7:amd64.\n",
      "Preparing to unpack .../14-libopenh264-7_2.4.1+dfsg-1_amd64.deb ...\n",
      "Unpacking libopenh264-7:amd64 (2.4.1+dfsg-1) ...\n",
      "Selecting previously unselected package libswresample5:amd64.\n",
      "Preparing to unpack .../15-libswresample5_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libswresample5:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Selecting previously unselected package libvo-amrwbenc0:amd64.\n",
      "Preparing to unpack .../16-libvo-amrwbenc0_0.1.3-2build1_amd64.deb ...\n",
      "Unpacking libvo-amrwbenc0:amd64 (0.1.3-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libvvenc1.12:amd64.\n",
      "Preparing to unpack .../17-libvvenc1.12_1.12.0-0build5~ubuntu2404_amd64.deb ...\n",
      "Unpacking libvvenc1.12:amd64 (1.12.0-0build5~ubuntu2404) ...\n",
      "Selecting previously unselected package libxavs2-13:amd64.\n",
      "Preparing to unpack .../18-libxavs2-13_1.3-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libxavs2-13:amd64 (1.3-1) ...\n",
      "Selecting previously unselected package libavcodec61:amd64.\n",
      "Preparing to unpack .../19-libavcodec61_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking libavcodec61:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libavc1394-0:amd64.\n",
      "Preparing to unpack .../20-libavc1394-0_0.5.4-5build3_amd64.deb ...\n",
      "Unpacking libavc1394-0:amd64 (0.5.4-5build3) ...\n",
      "Selecting previously unselected package libunibreak5:amd64.\n",
      "Preparing to unpack .../21-libunibreak5_5.1-2build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libunibreak5:amd64 (5.1-2build1) ...\n",
      "Selecting previously unselected package libass9:amd64.\n",
      "Preparing to unpack .../22-libass9_1%3a0.17.1-2build1_amd64.deb ...\n",
      "Unpacking libass9:amd64 (1:0.17.1-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libdvdread8t64:amd64.\n",
      "Preparing to unpack .../23-libdvdread8t64_6.1.3-1.1build1_amd64.deb ...\n",
      "Unpacking libdvdread8t64:amd64 (6.1.3-1.1build1) ...\n",
      "Selecting previously unselected package libdvdnav4:amd64.\n",
      "Preparing to unpack .../24-libdvdnav4_6.1.1-3build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libdvdnav4:amd64 (6.1.1-3build1) ...\n",
      "Selecting previously unselected package libtalloc2:amd64.\n",
      "Preparing to unpack .../25-libtalloc2_2.4.2-1build2_amd64.deb ...\n",
      "Unpacking libtalloc2:amd64 (2.4.2-1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libtdb1:amd64.\n",
      "Preparing to unpack .../26-libtdb1_1.4.10-1build1_amd64.deb ...\n",
      "Unpacking libtdb1:amd64 (1.4.10-1build1) ...\n",
      "Selecting previously unselected package libtevent0t64:amd64.\n",
      "Preparing to unpack .../27-libtevent0t64_0.16.1-2build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libtevent0t64:amd64 (0.16.1-2build1) ...\n",
      "Selecting previously unselected package libldb2:amd64.\n",
      "Preparing to unpack .../28-libldb2_2%3a2.8.0+samba4.19.5+dfsg-4ubuntu9.4_amd64.deb ...\n",
      "Unpacking libldb2:amd64 (2:2.8.0+samba4.19.5+dfsg-4ubuntu9.4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libwbclient0:amd64.\n",
      "Preparing to unpack .../29-libwbclient0_2%3a4.19.5+dfsg-4ubuntu9.4_amd64.deb ...\n",
      "Unpacking libwbclient0:amd64 (2:4.19.5+dfsg-4ubuntu9.4) ...\n",
      "Selecting previously unselected package samba-libs:amd64.\n",
      "Preparing to unpack .../30-samba-libs_2%3a4.19.5+dfsg-4ubuntu9.4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking samba-libs:amd64 (2:4.19.5+dfsg-4ubuntu9.4) ...\n",
      "Selecting previously unselected package libsmbclient0:amd64.\n",
      "Preparing to unpack .../31-libsmbclient0_2%3a4.19.5+dfsg-4ubuntu9.4_amd64.deb ...\n",
      "Unpacking libsmbclient0:amd64 (2:4.19.5+dfsg-4ubuntu9.4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libavformat61:amd64.\n",
      "Preparing to unpack .../32-libavformat61_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking libavformat61:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Selecting previously unselected package libbs2b0:amd64.\n",
      "Preparing to unpack .../33-libbs2b0_3.1.0+dfsg-7build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking libbs2b0:amd64 (3.1.0+dfsg-7build1) ...\n",
      "Selecting previously unselected package libflite1:amd64.\n",
      "Preparing to unpack .../34-libflite1_2.2-6build3_amd64.deb ...\n",
      "Unpacking libflite1:amd64 (2.2-6build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libserd-0-0:amd64.\n",
      "Preparing to unpack .../35-libserd-0-0_0.32.2-1_amd64.deb ...\n",
      "Unpacking libserd-0-0:amd64 (0.32.2-1) ...\n",
      "Selecting previously unselected package libzix-0-0:amd64.\n",
      "Preparing to unpack .../36-libzix-0-0_0.4.2-2build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libzix-0-0:amd64 (0.4.2-2build1) ...\n",
      "Selecting previously unselected package libsord-0-0:amd64.\n",
      "Preparing to unpack .../37-libsord-0-0_0.16.16-2build1_amd64.deb ...\n",
      "Unpacking libsord-0-0:amd64 (0.16.16-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Selecting previously unselected package libsratom-0-0:amd64.\n",
      "Preparing to unpack .../38-libsratom-0-0_0.6.16-1build1_amd64.deb ...\n",
      "Unpacking libsratom-0-0:amd64 (0.6.16-1build1) ...\n",
      "Selecting previously unselected package liblilv-0-0:amd64.\n",
      "Preparing to unpack .../39-liblilv-0-0_0.24.22-1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking liblilv-0-0:amd64 (0.24.22-1build1) ...\n",
      "Selecting previously unselected package libmysofa1:amd64.\n",
      "Preparing to unpack .../40-libmysofa1_1.3.2+dfsg-2ubuntu2_amd64.deb ...\n",
      "Unpacking libmysofa1:amd64 (1.3.2+dfsg-2ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libpostproc58:amd64.\n",
      "Preparing to unpack .../41-libpostproc58_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking libpostproc58:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Selecting previously unselected package libfftw3-double3:amd64.\n",
      "Preparing to unpack .../42-libfftw3-double3_3.3.10-1ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libfftw3-double3:amd64 (3.3.10-1ubuntu3) ...\n",
      "Selecting previously unselected package libsamplerate0:amd64.\n",
      "Preparing to unpack .../43-libsamplerate0_0.2.2-4build1_amd64.deb ...\n",
      "Unpacking libsamplerate0:amd64 (0.2.2-4build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package librubberband2:amd64.\n",
      "Preparing to unpack .../44-librubberband2_3.3.0+dfsg-2build1_amd64.deb ...\n",
      "Unpacking librubberband2:amd64 (3.3.0+dfsg-2build1) ...\n",
      "Selecting previously unselected package libswscale8:amd64.\n",
      "Preparing to unpack .../45-libswscale8_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libswscale8:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Selecting previously unselected package libvidstab1.1:amd64.\n",
      "Preparing to unpack .../46-libvidstab1.1_1.1.0-2build1_amd64.deb ...\n",
      "Unpacking libvidstab1.1:amd64 (1.1.0-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libvmaf1:amd64.\n",
      "Preparing to unpack .../47-libvmaf1_2.3.1-0build2~noble_amd64.deb ...\n",
      "Unpacking libvmaf1:amd64 (2.3.1-0build2~noble) ...\n",
      "Selecting previously unselected package libzimg2:amd64.\n",
      "Preparing to unpack .../48-libzimg2_3.0.5+ds1-1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libzimg2:amd64 (3.0.5+ds1-1build1) ...\n",
      "Selecting previously unselected package libavfilter10:amd64.\n",
      "Preparing to unpack .../49-libavfilter10_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking libavfilter10:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libcaca0:amd64.\n",
      "Preparing to unpack .../50-libcaca0_0.99.beta20-4build2_amd64.deb ...\n",
      "Unpacking libcaca0:amd64 (0.99.beta20-4build2) ...\n",
      "Selecting previously unselected package libcdio19t64:amd64.\n",
      "Preparing to unpack .../51-libcdio19t64_2.1.0-4.1ubuntu1.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libcdio19t64:amd64 (2.1.0-4.1ubuntu1.2) ...\n",
      "Selecting previously unselected package libcdio-cdda2t64:amd64.\n",
      "Preparing to unpack .../52-libcdio-cdda2t64_10.2+2.0.1-1.1build2_amd64.deb ...\n",
      "Unpacking libcdio-cdda2t64:amd64 (10.2+2.0.1-1.1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libcdio-paranoia2t64:amd64.\n",
      "Preparing to unpack .../53-libcdio-paranoia2t64_10.2+2.0.1-1.1build2_amd64.deb ...\n",
      "Unpacking libcdio-paranoia2t64:amd64 (10.2+2.0.1-1.1build2) ...\n",
      "Selecting previously unselected package libiec61883-0:amd64.\n",
      "Preparing to unpack .../54-libiec61883-0_1.2.0-6build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Unpacking libiec61883-0:amd64 (1.2.0-6build1) ...\n",
      "Selecting previously unselected package libjack-jackd2-0:amd64.\n",
      "Preparing to unpack .../55-libjack-jackd2-0_1.9.21~dfsg-3ubuntu3_amd64.deb ...\n",
      "Unpacking libjack-jackd2-0:amd64 (1.9.21~dfsg-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libopenal-data.\n",
      "Preparing to unpack .../56-libopenal-data_1%3a1.23.1-4build1_all.deb ...\n",
      "Unpacking libopenal-data (1:1.23.1-4build1) ...\n",
      "Selecting previously unselected package libsndio7.0:amd64.\n",
      "Preparing to unpack .../57-libsndio7.0_1.9.0-0.3build3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libsndio7.0:amd64 (1.9.0-0.3build3) ...\n",
      "Selecting previously unselected package libopenal1:amd64.\n",
      "Preparing to unpack .../58-libopenal1_1%3a1.23.1-4build1_amd64.deb ...\n",
      "Unpacking libopenal1:amd64 (1:1.23.1-4build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libasyncns0:amd64.\n",
      "Preparing to unpack .../59-libasyncns0_0.8-6build4_amd64.deb ...\n",
      "Unpacking libasyncns0:amd64 (0.8-6build4) ...\n",
      "Selecting previously unselected package libflac12t64:amd64.\n",
      "Preparing to unpack .../60-libflac12t64_1.4.3+ds-2.1ubuntu2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking libflac12t64:amd64 (1.4.3+ds-2.1ubuntu2) ...\n",
      "Selecting previously unselected package libsndfile1:amd64.\n",
      "Preparing to unpack .../61-libsndfile1_1.2.2-1ubuntu5.24.04.1_amd64.deb ...\n",
      "Unpacking libsndfile1:amd64 (1.2.2-1ubuntu5.24.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package libpulse0:amd64.\n",
      "Preparing to unpack .../62-libpulse0_1%3a16.1+dfsg1-2ubuntu10.1_amd64.deb ...\n",
      "Unpacking libpulse0:amd64 (1:16.1+dfsg1-2ubuntu10.1) ...\n",
      "Selecting previously unselected package libdecor-0-0:amd64.\n",
      "Preparing to unpack .../63-libdecor-0-0_0.2.2-1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking libdecor-0-0:amd64 (0.2.2-1build2) ...\n",
      "Selecting previously unselected package libsdl2-2.0-0:amd64.\n",
      "Preparing to unpack .../64-libsdl2-2.0-0_2.30.0+dfsg-1ubuntu3.1_amd64.deb ...\n",
      "Unpacking libsdl2-2.0-0:amd64 (2.30.0+dfsg-1ubuntu3.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libxv1:amd64.\n",
      "Preparing to unpack .../65-libxv1_2%3a1.0.11-1.1build1_amd64.deb ...\n",
      "Unpacking libxv1:amd64 (2:1.0.11-1.1build1) ...\n",
      "Selecting previously unselected package libavdevice61:amd64.\n",
      "Preparing to unpack .../66-libavdevice61_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libavdevice61:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Selecting previously unselected package libplacebo338:amd64.\n",
      "Preparing to unpack .../67-libplacebo338_6.338.2-2build1_amd64.deb ...\n",
      "Unpacking libplacebo338:amd64 (6.338.2-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Selecting previously unselected package ffmpeg.\n",
      "Preparing to unpack .../68-ffmpeg_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking ffmpeg (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Preparing to unpack .../69-libswscale-dev_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libswscale-dev:amd64 (7:7.1.1-0build1~ubuntu2404) over (7:6.1.1-3ubuntu5) ...\n",
      "Preparing to unpack .../70-libavformat-dev_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking libavformat-dev:amd64 (7:7.1.1-0build1~ubuntu2404) over (7:6.1.1-3ubuntu5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Preparing to unpack .../71-libavcodec-dev_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking libavcodec-dev:amd64 (7:7.1.1-0build1~ubuntu2404) over (7:6.1.1-3ubuntu5) ...\n",
      "Preparing to unpack .../72-libswresample-dev_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking libswresample-dev:amd64 (7:7.1.1-0build1~ubuntu2404) over (7:6.1.1-3ubuntu5) ...\n",
      "Preparing to unpack .../73-libavutil-dev_7%3a7.1.1-0build1~ubuntu2404_amd64.deb ...\n",
      "Unpacking libavutil-dev:amd64 (7:7.1.1-0build1~ubuntu2404) over (7:6.1.1-3ubuntu5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Selecting previously unselected package libdecor-0-plugin-1-gtk:amd64.\n",
      "Preparing to unpack .../74-libdecor-0-plugin-1-gtk_0.2.2-1build2_amd64.deb ...\n",
      "Unpacking libdecor-0-plugin-1-gtk:amd64 (0.2.2-1build2) ...\n",
      "Setting up libavc1394-0:amd64 (0.5.4-5build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up liblmdb0:amd64 (0.9.31-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libfdk-aac2:amd64 (2.0.2-3~ubuntu4) ...\n",
      "Setting up libdvdread8t64:amd64 (6.1.3-1.1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libvo-amrwbenc0:amd64 (0.1.3-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libvmaf1:amd64 (2.3.1-0build2~noble) ...\n",
      "Setting up libzix-0-0:amd64 (0.4.2-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libtdb1:amd64 (1.4.10-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libaribb24-0t64:amd64 (1.0.3-2.1build2) ...\n",
      "Setting up libmysofa1:amd64 (1.3.2+dfsg-2ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libcdio19t64:amd64 (2.1.0-4.1ubuntu1.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libilbc3:amd64 (3.0.4-0build1~noble) ...\n",
      "Setting up libavutil59:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libwbclient0:amd64 (2:4.19.5+dfsg-4ubuntu9.4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libcdio-cdda2t64:amd64 (10.2+2.0.1-1.1build2) ...\n",
      "Setting up libpostproc58:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libtalloc2:amd64 (2.4.2-1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libasound2-data (1.2.11-1ubuntu0.1) ...\n",
      "Setting up libopencore-amrwb0:amd64 (0.1.6-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libswresample5:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libswscale8:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Setting up libplacebo338:amd64 (6.338.2-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libasound2t64:amd64 (1.2.11-1ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libslang2:amd64 (2.3.3-3build2) ...\n",
      "Setting up libcdio-paranoia2t64:amd64 (10.2+2.0.1-1.1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libxv1:amd64 (2:1.0.11-1.1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libunibreak5:amd64 (5.1-2build1) ...\n",
      "Setting up libfftw3-double3:amd64 (3.3.10-1ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libopenh264-7:amd64 (2.4.1+dfsg-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libsndio7.0:amd64 (1.9.0-0.3build3) ...\n",
      "Setting up libvidstab1.1:amd64 (1.1.0-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up alsa-topology-conf (1.2.5.1-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libflite1:amd64 (2.2-6build3) ...\n",
      "Setting up libxavs2-13:amd64 (1.3-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libasyncns0:amd64 (0.8-6build4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libbs2b0:amd64 (3.1.0+dfsg-7build1) ...\n",
      "Setting up libdavs2-16:amd64 (1.6-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libopencore-amrnb0:amd64 (0.1.6-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libdecor-0-0:amd64 (0.2.2-1build2) ...\n",
      "Setting up libzimg2:amd64 (3.0.5+ds1-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libopenal-data (1:1.23.1-4build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libflac12t64:amd64 (1.4.3+ds-2.1ubuntu2) ...\n",
      "Setting up libsndfile1:amd64 (1.2.2-1ubuntu5.24.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libvvenc1.12:amd64 (1.12.0-0build5~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libsamplerate0:amd64 (0.2.2-4build1) ...\n",
      "Setting up libdecor-0-plugin-1-gtk:amd64 (0.2.2-1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libtevent0t64:amd64 (0.16.1-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libdvdnav4:amd64 (6.1.1-3build1) ...\n",
      "Setting up libiec61883-0:amd64 (1.2.0-6build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libserd-0-0:amd64 (0.32.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libkvazaar7:amd64 (2.2.0-0build1~noble) ...\n",
      "Setting up libavutil-dev:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libcaca0:amd64 (0.99.beta20-4build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up alsa-ucm-conf (1.2.10-1ubuntu5.7) ...\n",
      "Setting up libpulse0:amd64 (1:16.1+dfsg1-2ubuntu10.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libswresample-dev:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libopenal1:amd64 (1:1.23.1-4build1) ...\n",
      "Setting up libass9:amd64 (1:0.17.1-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libswscale-dev:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libavcodec61:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Setting up librubberband2:amd64 (3.3.0+dfsg-2build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libjack-jackd2-0:amd64 (1.9.21~dfsg-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libsord-0-0:amd64 (0.16.16-2build1) ...\n",
      "Setting up libsratom-0-0:amd64 (0.6.16-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libsdl2-2.0-0:amd64 (2.30.0+dfsg-1ubuntu3.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libldb2:amd64 (2:2.8.0+samba4.19.5+dfsg-4ubuntu9.4) ...\n",
      "Setting up liblilv-0-0:amd64 (0.24.22-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libavcodec-dev:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up samba-libs:amd64 (2:4.19.5+dfsg-4ubuntu9.4) ...\n",
      "Setting up libsmbclient0:amd64 (2:4.19.5+dfsg-4ubuntu9.4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up libavformat61:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libavfilter10:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Setting up libavdevice61:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libavformat-dev:amd64 (7:7.1.1-0build1~ubuntu2404) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8Setting up ffmpeg (7:7.1.1-0build1~ubuntu2404) ...\n",
      "Processing triggers for libc-bin (2.39-0ubuntu8.5) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!add-apt-repository ppa:ubuntuhandbook1/ffmpeg7 -y # install PPA which contains ffmpeg 7.x\n",
    "!apt update && apt install ffmpeg -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxCc3CARwUjN"
   },
   "source": [
    "## Install LeRobot v0.4.1\n",
    "This cell clones the `lerobot` repository from Hugging Face, and installs the package in editable mode. Extra Features: To install additional dependencies for training SmolVLA or Pi models, refer to the [LeRobot offical page](https://huggingface.co/docs/lerobot/index). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgLu7QT5tUik",
    "outputId": "e46913b8-1977-48a5-a851-d8c69602419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lerobot'...\n",
      "remote: Enumerating objects: 44329, done.\u001b[K\n",
      "remote: Counting objects: 100% (323/323), done.\u001b[K\n",
      "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
      "remote: Total 44329 (delta 247), reused 138 (delta 135), pack-reused 44006 (from 3)\u001b[K\n",
      "Receiving objects: 100% (44329/44329), 220.84 MiB | 74.29 MiB/s, done.\n",
      "Resolving deltas: 100% (28591/28591), done.\n",
      "Switched to a new branch 'v0.4.1'\n",
      "Obtaining file:///workspace/lerobot\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets<4.2.0,>=4.0.0 (from lerobot==0.4.1)\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting diffusers<0.36.0,>=0.27.2 (from lerobot==0.4.1)\n",
      "  Downloading diffusers-0.35.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting huggingface-hub<0.36.0,>=0.34.2 (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting accelerate<2.0.0,>=1.10.0 (from lerobot==0.4.1)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=71.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (80.9.0)\n",
      "Collecting cmake<4.2.0,>=3.29.0.1 (from lerobot==0.4.1)\n",
      "  Downloading cmake-4.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting einops<0.9.0,>=0.8.0 (from lerobot==0.4.1)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting opencv-python-headless<4.13.0,>=4.9.0 (from lerobot==0.4.1)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting av<16.0.0,>=15.0.0 (from lerobot==0.4.1)\n",
      "  Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting jsonlines<5.0.0,>=4.0.0 (from lerobot==0.4.1)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: packaging<26.0,>=24.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (25.0)\n",
      "Collecting pynput<1.9.0,>=1.7.7 (from lerobot==0.4.1)\n",
      "  Downloading pynput-1.8.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting pyserial<4.0,>=3.5 (from lerobot==0.4.1)\n",
      "  Downloading pyserial-3.5-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting wandb<0.22.0,>=0.20.0 (from lerobot==0.4.1)\n",
      "  Downloading wandb-0.21.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (2.7.1+rocm7.0.0.lw.git698b58a9)\n",
      "Collecting torchcodec<0.6.0,>=0.2.1 (from lerobot==0.4.1)\n",
      "  Downloading torchcodec-0.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.22.1+rocm7.0.0.git59a3e1f9)\n",
      "Collecting draccus==0.10.0 (from lerobot==0.4.1)\n",
      "  Downloading draccus-0.10.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting gymnasium<2.0.0,>=1.1.1 (from lerobot==0.4.1)\n",
      "  Downloading gymnasium-1.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting rerun-sdk<0.27.0,>=0.24.0 (from lerobot==0.4.1)\n",
      "  Downloading rerun_sdk-0.26.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting deepdiff<9.0.0,>=7.0.1 (from lerobot==0.4.1)\n",
      "  Downloading deepdiff-8.6.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting imageio<3.0.0,>=2.34.0 (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1)\n",
      "  Downloading imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting termcolor<4.0.0,>=2.4.0 (from lerobot==0.4.1)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting mergedeep~=1.3 (from draccus==0.10.0->lerobot==0.4.1)\n",
      "  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (6.0.2)\n",
      "Collecting pyyaml-include~=1.4 (from draccus==0.10.0->lerobot==0.4.1)\n",
      "  Downloading pyyaml_include-1.4.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting toml~=0.10 (from draccus==0.10.0->lerobot==0.4.1)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect~=0.9.0 (from draccus==0.10.0->lerobot==0.4.1)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (2.3.3)\n",
      "Requirement already satisfied: psutil in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (7.1.3)\n",
      "Collecting safetensors>=0.4.3 (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.19.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (4.67.1)\n",
      "Collecting xxhash (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.9.0)\n",
      "Collecting orderly-set<6,>=5.4.1 (from deepdiff<9.0.0,>=7.0.1->lerobot==0.4.1)\n",
      "  Downloading orderly_set-5.5.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting importlib_metadata (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting regex!=2019.12.17 (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: Pillow in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (11.3.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting cloudpickle>=1.2.0 (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (4.15.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting InquirerPy==0.3.4 (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1)\n",
      "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting hf-transfer>=0.1.4 (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1)\n",
      "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (3.0.52)\n",
      "Collecting imageio-ffmpeg (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1)\n",
      "  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/venv/lib/python3.12/site-packages (from jsonlines<5.0.0,>=4.0.0->lerobot==0.4.1) (25.4.0)\n",
      "Collecting numpy>=1.17 (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: wcwidth in /opt/venv/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.2.14)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.17.0)\n",
      "Collecting evdev>=1.3 (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1)\n",
      "  Downloading evdev-1.9.2.tar.gz (33 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-xlib>=0.17 (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1)\n",
      "  Downloading python_xlib-0.33-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.1.6)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.3.1+rocm7.0.0.git9c7bc0a3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.3.1+rocm7.0.0.git9c7bc0a3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.4.1)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting click>=8.0.1 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.5.1)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading sentry_sdk-2.47.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.11.12)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.3.0)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot==0.4.1)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.12/site-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Downloading draccus-0.10.0-py3-none-any.whl (71 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cmake-4.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (29.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Downloading deepdiff-8.6.1-py3-none-any.whl (91 kB)\n",
      "Downloading diffusers-0.35.2-py3-none-any.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading gymnasium-1.2.2-py3-none-any.whl (952 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
      "Downloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orderly_set-5.5.0-py3-none-any.whl (13 kB)\n",
      "Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
      "Downloading pynput-1.8.1-py2.py3-none-any.whl (91 kB)\n",
      "Downloading pyserial-3.5-py2.py3-none-any.whl (90 kB)\n",
      "Downloading pyyaml_include-1.4.1-py3-none-any.whl (19 kB)\n",
      "Downloading rerun_sdk-0.26.2-cp39-abi3-manylinux_2_28_x86_64.whl (98.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading torchcodec-0.5-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading wandb-0.21.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m215.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_xlib-0.33-py2.py3-none-any.whl (182 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading sentry_sdk-2.47.0-py2.py3-none-any.whl (411 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Building wheels for collected packages: lerobot, evdev\n",
      "  Building editable for lerobot (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lerobot: filename=lerobot-0.4.1-0.editable-py3-none-any.whl size=15631 sha256=a4a54b1c482a2f6c7e4147399356cb8c31708adabbf4d417a720d843d512a7fe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gs6257jl/wheels/05/0a/0d/80a4c08845345c44fe1e5f70929884983b90d85f46a77f7601\n",
      "  Building wheel for evdev (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for evdev: filename=evdev-1.9.2-cp312-cp312-linux_x86_64.whl size=114632 sha256=5c4a71eb7e0e47d18383297cca467547b90f6034d496abd3bddb29f84891d6f4\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/f7/62/6b6f5201f6536a3a9e38c94726e03a3b2bded0aaf7782b12d7\n",
      "Successfully built lerobot evdev\n",
      "Installing collected packages: pyserial, farama-notifications, zipp, xxhash, typing-inspection, torchcodec, toml, termcolor, smmap, sentry-sdk, safetensors, regex, pyyaml-include, python-xlib, pydantic-core, pyarrow, protobuf, propcache, pfzy, orderly-set, numpy, mypy-extensions, multidict, mergedeep, jsonlines, imageio-ffmpeg, hf-xet, hf-transfer, frozenlist, evdev, einops, dill, cmake, cloudpickle, click, av, annotated-types, aiohappyeyeballs, yarl, typing-inspect, rerun-sdk, pynput, pydantic, opencv-python-headless, multiprocess, InquirerPy, importlib_metadata, imageio, huggingface-hub, gymnasium, gitdb, deepdiff, aiosignal, gitpython, draccus, diffusers, aiohttp, accelerate, wandb, datasets, lerobot\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/61\u001b[0m [protobuf]core]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.3━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/61\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling numpy-2.3.3:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/61\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.3━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/61\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61/61\u001b[0m [lerobot]9/61\u001b[0m [datasets]e]-hub]ta]ess]\n",
      "\u001b[1A\u001b[2KSuccessfully installed InquirerPy-0.3.4 accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 av-15.1.0 click-8.3.1 cloudpickle-3.1.2 cmake-4.1.3 datasets-4.1.1 deepdiff-8.6.1 diffusers-0.35.2 dill-0.4.0 draccus-0.10.0 einops-0.8.1 evdev-1.9.2 farama-notifications-0.0.4 frozenlist-1.8.0 gitdb-4.0.12 gitpython-3.1.45 gymnasium-1.2.2 hf-transfer-0.1.9 hf-xet-1.2.0 huggingface-hub-0.35.3 imageio-2.37.2 imageio-ffmpeg-0.6.0 importlib_metadata-8.7.0 jsonlines-4.0.0 lerobot-0.4.1 mergedeep-1.3.4 multidict-6.7.0 multiprocess-0.70.16 mypy-extensions-1.1.0 numpy-2.2.6 opencv-python-headless-4.12.0.88 orderly-set-5.5.0 pfzy-0.3.4 propcache-0.4.1 protobuf-6.33.1 pyarrow-22.0.0 pydantic-2.12.5 pydantic-core-2.41.5 pynput-1.8.1 pyserial-3.5 python-xlib-0.33 pyyaml-include-1.4.1 regex-2025.11.3 rerun-sdk-0.26.2 safetensors-0.7.0 sentry-sdk-2.47.0 smmap-5.0.2 termcolor-3.2.0 toml-0.10.2 torchcodec-0.5 typing-inspect-0.9.0 typing-inspection-0.4.2 wandb-0.21.4 xxhash-3.6.0 yarl-1.22.0 zipp-3.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/lerobot.git\n",
    "!cd lerobot && git checkout -b v0.4.1 v0.4.1 # let’s synchronize using this version\n",
    "!cd lerobot && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8Sn2wG4wldo"
   },
   "source": [
    "## Weights & Biases login\n",
    "This cell install and log into Weights & Biases (wandb) to enable experiment tracking and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PolVM_movEvp",
    "outputId": "1769c6bd-8644-4b65-84c7-4f020b234c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/venv/lib/python3.12/site-packages (0.21.4)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/venv/lib/python3.12/site-packages (from wandb) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /opt/venv/lib/python3.12/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (6.33.1)\n",
      "Requirement already satisfied: pydantic<3 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in /opt/venv/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb) (2.47.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /opt/venv/lib/python3.12/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnaoki-takada\u001b[0m (\u001b[33mnaoki-takada-matsuo-lab\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login into Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PolVM_movEvp",
    "outputId": "1769c6bd-8644-4b65-84c7-4f020b234c92"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkzTo4mNwxaC"
   },
   "source": [
    "## Start Training Models with LeRobot\n",
    "\n",
    "This cell uses the lerobot-train CLI from the lerobot library to train a robot control policy.  \n",
    "\n",
    "Make sure to adjust the following arguments to your setup:\n",
    "\n",
    "1. `--dataset.repo_id=YOUR_HF_USERNAME/YOUR_DATASET`:  \n",
    "   Replace this with the Hugging Face Hub repo ID where your dataset is stored, e.g., `lerobot/svla_so100_pickplace`.\n",
    "\n",
    "2. `--policy.type=act`:  \n",
    "   Specifies the policy configuration to use. `act` refers to [configuration_act.py](../lerobot/common/policies/act/configuration_act.py), which will automatically adapt to your dataset’s setup (e.g., number of motors and cameras).\n",
    "\n",
    "3. `--output_dir=outputs/train/...`:  \n",
    "   Directory where training logs and model checkpoints will be saved.\n",
    "\n",
    "4. `--job_name=...`:  \n",
    "   A name for this training job, used for logging and Weights & Biases.The name typically includes the model type (e.g., act, smolvla), the dataset name, and additional descriptive tags.\n",
    "\n",
    "5. `--policy.device=cuda`:  \n",
    "   Use `cuda` if training on an AMD or NVIDIA GPU. \n",
    "\n",
    "6. `--wandb.enable=true`:  \n",
    "   Enables Weights & Biases for visualizing training progress. You must be logged in via `wandb login` before running this.\n",
    "\n",
    "7. `--policy.push_to_hub=`:\n",
    "\n",
    "   Enables automatic uploading of the trained policy to the Hugging Face Hub. You must specify `--policy.repo_id` (e.g., ${HF_USER}/{REPO_NAME}) if it is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ufss6US6xbpi",
    "outputId": "cfd494c7-8689-419d-bb17-7f02a67b26d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2025-12-05 17:52:02 ot_train.py:163 {'batch_size': 64,\n",
      " 'checkpoint_path': None,\n",
      " 'dataset': {'episodes': None,\n",
      "             'image_transforms': {'enable': False,\n",
      "                                  'max_num_transforms': 3,\n",
      "                                  'random_order': False,\n",
      "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
      "                                                                            5.0],\n",
      "                                                                'translate': [0.05,\n",
      "                                                                              0.05]},\n",
      "                                                     'type': 'RandomAffine',\n",
      "                                                     'weight': 1.0},\n",
      "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
      "                                                                                   1.2]},\n",
      "                                                         'type': 'ColorJitter',\n",
      "                                                         'weight': 1.0},\n",
      "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
      "                                                                               1.2]},\n",
      "                                                       'type': 'ColorJitter',\n",
      "                                                       'weight': 1.0},\n",
      "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
      "                                                                     0.05]},\n",
      "                                                  'type': 'ColorJitter',\n",
      "                                                  'weight': 1.0},\n",
      "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
      "                                                                                   1.5]},\n",
      "                                                         'type': 'ColorJitter',\n",
      "                                                         'weight': 1.0},\n",
      "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
      "                                                                                 1.5]},\n",
      "                                                        'type': 'SharpnessJitter',\n",
      "                                                        'weight': 1.0}}},\n",
      "             'repo_id': 'nkmurst/amd_record-test3',\n",
      "             'revision': None,\n",
      "             'root': None,\n",
      "             'streaming': False,\n",
      "             'use_imagenet_stats': True,\n",
      "             'video_backend': 'torchcodec'},\n",
      " 'env': None,\n",
      " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
      " 'eval_freq': 20000,\n",
      " 'job_name': 'so101_petcap_smolvla2',\n",
      " 'log_freq': 200,\n",
      " 'num_workers': 4,\n",
      " 'optimizer': {'betas': [0.9, 0.95],\n",
      "               'eps': 1e-08,\n",
      "               'grad_clip_norm': 10.0,\n",
      "               'lr': 0.0001,\n",
      "               'type': 'adamw',\n",
      "               'weight_decay': 1e-10},\n",
      " 'output_dir': 'outputs/train/so101_petcap_smolvla2',\n",
      " 'policy': {'adapt_to_pi_aloha': False,\n",
      "            'add_image_special_tokens': False,\n",
      "            'attention_mode': 'cross_attn',\n",
      "            'chunk_size': 50,\n",
      "            'device': 'cuda',\n",
      "            'empty_cameras': 0,\n",
      "            'expert_width_multiplier': 0.75,\n",
      "            'freeze_vision_encoder': True,\n",
      "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.images.camera2': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.images.camera3': {'shape': [3,\n",
      "                                                                        256,\n",
      "                                                                        256],\n",
      "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
      "                               'observation.state': {'shape': [6],\n",
      "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
      "            'license': None,\n",
      "            'load_vlm_weights': True,\n",
      "            'max_action_dim': 32,\n",
      "            'max_period': 4.0,\n",
      "            'max_state_dim': 32,\n",
      "            'min_period': 0.004,\n",
      "            'n_action_steps': 50,\n",
      "            'n_obs_steps': 1,\n",
      "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
      "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
      "            'num_expert_layers': 0,\n",
      "            'num_steps': 10,\n",
      "            'num_vlm_layers': 16,\n",
      "            'optimizer_betas': [0.9, 0.95],\n",
      "            'optimizer_eps': 1e-08,\n",
      "            'optimizer_grad_clip_norm': 10.0,\n",
      "            'optimizer_lr': 0.0001,\n",
      "            'optimizer_weight_decay': 1e-10,\n",
      "            'output_features': {'action': {'shape': [6],\n",
      "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
      "            'pad_language_to': 'max_length',\n",
      "            'prefix_length': 0,\n",
      "            'pretrained_path': 'lerobot/smolvla_base',\n",
      "            'private': None,\n",
      "            'push_to_hub': False,\n",
      "            'repo_id': None,\n",
      "            'resize_imgs_with_padding': [512, 512],\n",
      "            'scheduler_decay_lr': 2.5e-06,\n",
      "            'scheduler_decay_steps': 30000,\n",
      "            'scheduler_warmup_steps': 1000,\n",
      "            'self_attn_every_n_layers': 2,\n",
      "            'tags': None,\n",
      "            'tokenizer_max_length': 48,\n",
      "            'train_expert_only': True,\n",
      "            'train_state_proj': True,\n",
      "            'type': 'smolvla',\n",
      "            'use_amp': False,\n",
      "            'use_cache': True,\n",
      "            'use_delta_joint_actions_aloha': False,\n",
      "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
      " 'rename_map': {'observation.images.side': 'observation.images.camera2',\n",
      "                'observation.images.top': 'observation.images.camera1'},\n",
      " 'resume': False,\n",
      " 'save_checkpoint': True,\n",
      " 'save_freq': 5000,\n",
      " 'scheduler': {'decay_lr': 2.5e-06,\n",
      "               'num_decay_steps': 30000,\n",
      "               'num_warmup_steps': 1000,\n",
      "               'peak_lr': 0.0001,\n",
      "               'type': 'cosine_decay_with_warmup'},\n",
      " 'seed': 1000,\n",
      " 'steps': 30000,\n",
      " 'use_policy_training_preset': True,\n",
      " 'wandb': {'disable_artifact': False,\n",
      "           'enable': True,\n",
      "           'entity': None,\n",
      "           'mode': None,\n",
      "           'notes': None,\n",
      "           'project': 'lerobot',\n",
      "           'run_id': None}}\n",
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "INFO 2025-12-05 17:52:04 db_utils.py:102 \u001b[1m\u001b[34mLogs will be synced with wandb.\u001b[0m\n",
      "INFO 2025-12-05 17:52:04 db_utils.py:103 Track this run --> \u001b[1m\u001b[33mhttps://wandb.ai/naoki-takada-matsuo-lab/lerobot/runs/7wtykcwl\u001b[0m\n",
      "INFO 2025-12-05 17:52:04 ot_train.py:183 Creating dataset\n",
      "INFO 2025-12-05 17:52:04 ot_train.py:202 Creating policy\n",
      "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
      "Reducing the number of VLM layers to 16 ...\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:247 Creating optimizer and scheduler\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:259 \u001b[1m\u001b[33mOutput dir:\u001b[0m outputs/train/so101_petcap_smolvla2\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:262 cfg.steps=30000 (30K)\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:263 dataset.num_frames=12005 (12K)\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:264 dataset.num_episodes=30\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:267 Effective batch size: 64 x 1 = 64\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:268 num_learnable_params=99880992 (100M)\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:269 num_total_params=450046176 (450M)\n",
      "INFO 2025-12-05 17:52:12 ot_train.py:324 Start offline training on a fixed dataset\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 17:53:45 ot_train.py:351 step:200 smpl:13K ep:32 epch:1.07 loss:0.081 grdn:0.557 lr:1.0e-05 updt_s:0.419 data_s:0.046\n",
      "WARNING 2025-12-05 17:53:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 17:53:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 17:54:58 ot_train.py:351 step:400 smpl:26K ep:64 epch:2.13 loss:0.047 grdn:0.425 lr:3.0e-05 updt_s:0.329 data_s:0.035\n",
      "WARNING 2025-12-05 17:54:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 17:54:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 17:56:12 ot_train.py:351 step:600 smpl:38K ep:96 epch:3.20 loss:0.038 grdn:0.436 lr:5.0e-05 updt_s:0.328 data_s:0.038\n",
      "WARNING 2025-12-05 17:56:12 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 17:56:12 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 17:57:24 ot_train.py:351 step:800 smpl:51K ep:128 epch:4.26 loss:0.036 grdn:0.452 lr:7.0e-05 updt_s:0.326 data_s:0.036\n",
      "WARNING 2025-12-05 17:57:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 17:57:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 17:58:38 ot_train.py:351 step:1K smpl:64K ep:160 epch:5.33 loss:0.036 grdn:0.453 lr:9.0e-05 updt_s:0.330 data_s:0.039\n",
      "WARNING 2025-12-05 17:58:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 17:58:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 17:59:52 ot_train.py:351 step:1K smpl:77K ep:192 epch:6.40 loss:0.033 grdn:0.413 lr:1.0e-04 updt_s:0.332 data_s:0.037\n",
      "WARNING 2025-12-05 17:59:52 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 17:59:52 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:01:07 ot_train.py:351 step:1K smpl:90K ep:224 epch:7.46 loss:0.031 grdn:0.396 lr:1.0e-04 updt_s:0.330 data_s:0.042\n",
      "WARNING 2025-12-05 18:01:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:01:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:02:21 ot_train.py:351 step:2K smpl:102K ep:256 epch:8.53 loss:0.029 grdn:0.375 lr:9.9e-05 updt_s:0.328 data_s:0.041\n",
      "WARNING 2025-12-05 18:02:21 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:02:21 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:03:35 ot_train.py:351 step:2K smpl:115K ep:288 epch:9.60 loss:0.026 grdn:0.351 lr:9.9e-05 updt_s:0.332 data_s:0.037\n",
      "WARNING 2025-12-05 18:03:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:03:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:04:49 ot_train.py:351 step:2K smpl:128K ep:320 epch:10.66 loss:0.025 grdn:0.326 lr:9.9e-05 updt_s:0.329 data_s:0.040\n",
      "WARNING 2025-12-05 18:04:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:04:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:06:03 ot_train.py:351 step:2K smpl:141K ep:352 epch:11.73 loss:0.024 grdn:0.316 lr:9.9e-05 updt_s:0.329 data_s:0.041\n",
      "WARNING 2025-12-05 18:06:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:06:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:07:17 ot_train.py:351 step:2K smpl:154K ep:384 epch:12.79 loss:0.023 grdn:0.293 lr:9.9e-05 updt_s:0.330 data_s:0.038\n",
      "WARNING 2025-12-05 18:07:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:07:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:08:31 ot_train.py:351 step:3K smpl:166K ep:416 epch:13.86 loss:0.022 grdn:0.310 lr:9.8e-05 updt_s:0.327 data_s:0.039\n",
      "WARNING 2025-12-05 18:08:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:08:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:09:44 ot_train.py:351 step:3K smpl:179K ep:448 epch:14.93 loss:0.021 grdn:0.290 lr:9.8e-05 updt_s:0.331 data_s:0.037\n",
      "WARNING 2025-12-05 18:09:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:09:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:10:58 ot_train.py:351 step:3K smpl:192K ep:480 epch:15.99 loss:0.020 grdn:0.293 lr:9.8e-05 updt_s:0.328 data_s:0.039\n",
      "WARNING 2025-12-05 18:10:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:10:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:12:18 ot_train.py:351 step:3K smpl:205K ep:512 epch:17.06 loss:0.019 grdn:0.271 lr:9.7e-05 updt_s:0.330 data_s:0.072\n",
      "WARNING 2025-12-05 18:12:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:12:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:13:33 ot_train.py:351 step:3K smpl:218K ep:544 epch:18.13 loss:0.019 grdn:0.289 lr:9.7e-05 updt_s:0.330 data_s:0.039\n",
      "WARNING 2025-12-05 18:13:33 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:13:33 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:14:49 ot_train.py:351 step:4K smpl:230K ep:576 epch:19.19 loss:0.019 grdn:0.280 lr:9.7e-05 updt_s:0.332 data_s:0.046\n",
      "WARNING 2025-12-05 18:14:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:14:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:16:03 ot_train.py:351 step:4K smpl:243K ep:608 epch:20.26 loss:0.017 grdn:0.260 lr:9.6e-05 updt_s:0.329 data_s:0.040\n",
      "WARNING 2025-12-05 18:16:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:16:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:17:19 ot_train.py:351 step:4K smpl:256K ep:640 epch:21.32 loss:0.017 grdn:0.256 lr:9.6e-05 updt_s:0.331 data_s:0.047\n",
      "WARNING 2025-12-05 18:17:19 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:17:19 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:18:32 ot_train.py:351 step:4K smpl:269K ep:672 epch:22.39 loss:0.016 grdn:0.247 lr:9.6e-05 updt_s:0.330 data_s:0.038\n",
      "WARNING 2025-12-05 18:18:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:18:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:19:47 ot_train.py:351 step:4K smpl:282K ep:704 epch:23.46 loss:0.016 grdn:0.244 lr:9.5e-05 updt_s:0.330 data_s:0.039\n",
      "WARNING 2025-12-05 18:19:47 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:19:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:21:02 ot_train.py:351 step:5K smpl:294K ep:736 epch:24.52 loss:0.016 grdn:0.253 lr:9.5e-05 updt_s:0.330 data_s:0.044\n",
      "WARNING 2025-12-05 18:21:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:21:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:22:16 ot_train.py:351 step:5K smpl:307K ep:768 epch:25.59 loss:0.015 grdn:0.252 lr:9.4e-05 updt_s:0.329 data_s:0.041\n",
      "WARNING 2025-12-05 18:22:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:22:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:23:30 ot_train.py:351 step:5K smpl:320K ep:800 epch:26.66 loss:0.014 grdn:0.248 lr:9.4e-05 updt_s:0.330 data_s:0.041\n",
      "WARNING 2025-12-05 18:23:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:23:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-05 18:23:30 ot_train.py:361 Checkpoint policy after step 5000\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:24:48 ot_train.py:351 step:5K smpl:333K ep:832 epch:27.72 loss:0.014 grdn:0.236 lr:9.3e-05 updt_s:0.333 data_s:0.039\n",
      "WARNING 2025-12-05 18:24:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:24:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:26:04 ot_train.py:351 step:5K smpl:346K ep:864 epch:28.79 loss:0.014 grdn:0.233 lr:9.3e-05 updt_s:0.333 data_s:0.044\n",
      "WARNING 2025-12-05 18:26:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:26:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:27:18 ot_train.py:351 step:6K smpl:358K ep:896 epch:29.85 loss:0.014 grdn:0.230 lr:9.2e-05 updt_s:0.329 data_s:0.039\n",
      "WARNING 2025-12-05 18:27:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:27:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:28:31 ot_train.py:351 step:6K smpl:371K ep:928 epch:30.92 loss:0.013 grdn:0.221 lr:9.2e-05 updt_s:0.327 data_s:0.040\n",
      "WARNING 2025-12-05 18:28:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:28:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:29:46 ot_train.py:351 step:6K smpl:384K ep:960 epch:31.99 loss:0.013 grdn:0.227 lr:9.1e-05 updt_s:0.331 data_s:0.039\n",
      "WARNING 2025-12-05 18:29:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:29:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:31:01 ot_train.py:351 step:6K smpl:397K ep:992 epch:33.05 loss:0.012 grdn:0.216 lr:9.0e-05 updt_s:0.330 data_s:0.044\n",
      "WARNING 2025-12-05 18:31:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:31:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:32:19 ot_train.py:351 step:6K smpl:410K ep:1K epch:34.12 loss:0.012 grdn:0.226 lr:9.0e-05 updt_s:0.326 data_s:0.066\n",
      "WARNING 2025-12-05 18:32:19 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:32:19 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:33:34 ot_train.py:351 step:7K smpl:422K ep:1K epch:35.19 loss:0.013 grdn:0.213 lr:8.9e-05 updt_s:0.331 data_s:0.040\n",
      "WARNING 2025-12-05 18:33:34 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:33:34 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:34:48 ot_train.py:351 step:7K smpl:435K ep:1K epch:36.25 loss:0.012 grdn:0.205 lr:8.8e-05 updt_s:0.332 data_s:0.037\n",
      "WARNING 2025-12-05 18:34:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:34:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:36:03 ot_train.py:351 step:7K smpl:448K ep:1K epch:37.32 loss:0.011 grdn:0.214 lr:8.8e-05 updt_s:0.332 data_s:0.044\n",
      "WARNING 2025-12-05 18:36:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:36:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:37:18 ot_train.py:351 step:7K smpl:461K ep:1K epch:38.38 loss:0.012 grdn:0.212 lr:8.7e-05 updt_s:0.330 data_s:0.043\n",
      "WARNING 2025-12-05 18:37:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:37:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:38:34 ot_train.py:351 step:7K smpl:474K ep:1K epch:39.45 loss:0.011 grdn:0.202 lr:8.6e-05 updt_s:0.333 data_s:0.041\n",
      "WARNING 2025-12-05 18:38:34 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:38:34 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:39:48 ot_train.py:351 step:8K smpl:486K ep:1K epch:40.52 loss:0.011 grdn:0.202 lr:8.6e-05 updt_s:0.330 data_s:0.042\n",
      "WARNING 2025-12-05 18:39:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:39:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:41:04 ot_train.py:351 step:8K smpl:499K ep:1K epch:41.58 loss:0.010 grdn:0.195 lr:8.5e-05 updt_s:0.334 data_s:0.043\n",
      "WARNING 2025-12-05 18:41:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:41:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:42:18 ot_train.py:351 step:8K smpl:512K ep:1K epch:42.65 loss:0.010 grdn:0.196 lr:8.4e-05 updt_s:0.329 data_s:0.041\n",
      "WARNING 2025-12-05 18:42:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:42:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:43:32 ot_train.py:351 step:8K smpl:525K ep:1K epch:43.72 loss:0.010 grdn:0.200 lr:8.3e-05 updt_s:0.330 data_s:0.040\n",
      "WARNING 2025-12-05 18:43:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:43:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:44:47 ot_train.py:351 step:8K smpl:538K ep:1K epch:44.78 loss:0.010 grdn:0.186 lr:8.3e-05 updt_s:0.329 data_s:0.041\n",
      "WARNING 2025-12-05 18:44:47 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:44:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:46:01 ot_train.py:351 step:9K smpl:550K ep:1K epch:45.85 loss:0.010 grdn:0.192 lr:8.2e-05 updt_s:0.333 data_s:0.040\n",
      "WARNING 2025-12-05 18:46:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:46:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:47:17 ot_train.py:351 step:9K smpl:563K ep:1K epch:46.91 loss:0.010 grdn:0.193 lr:8.1e-05 updt_s:0.331 data_s:0.046\n",
      "WARNING 2025-12-05 18:47:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:47:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:48:34 ot_train.py:351 step:9K smpl:576K ep:1K epch:47.98 loss:0.010 grdn:0.189 lr:8.0e-05 updt_s:0.333 data_s:0.047\n",
      "WARNING 2025-12-05 18:48:34 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:48:34 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:49:50 ot_train.py:351 step:9K smpl:589K ep:1K epch:49.05 loss:0.009 grdn:0.187 lr:7.9e-05 updt_s:0.334 data_s:0.044\n",
      "WARNING 2025-12-05 18:49:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:49:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:51:05 ot_train.py:351 step:9K smpl:602K ep:2K epch:50.11 loss:0.009 grdn:0.182 lr:7.9e-05 updt_s:0.332 data_s:0.044\n",
      "WARNING 2025-12-05 18:51:05 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:51:05 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:52:25 ot_train.py:351 step:10K smpl:614K ep:2K epch:51.18 loss:0.009 grdn:0.185 lr:7.8e-05 updt_s:0.331 data_s:0.066\n",
      "WARNING 2025-12-05 18:52:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:52:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:53:39 ot_train.py:351 step:10K smpl:627K ep:2K epch:52.24 loss:0.009 grdn:0.179 lr:7.7e-05 updt_s:0.330 data_s:0.038\n",
      "WARNING 2025-12-05 18:53:39 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:53:39 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:54:55 ot_train.py:351 step:10K smpl:640K ep:2K epch:53.31 loss:0.008 grdn:0.174 lr:7.6e-05 updt_s:0.329 data_s:0.047\n",
      "WARNING 2025-12-05 18:54:55 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:54:55 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-05 18:54:55 ot_train.py:361 Checkpoint policy after step 10000\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:56:12 ot_train.py:351 step:10K smpl:653K ep:2K epch:54.38 loss:0.008 grdn:0.169 lr:7.5e-05 updt_s:0.329 data_s:0.040\n",
      "WARNING 2025-12-05 18:56:12 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:56:12 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:57:27 ot_train.py:351 step:10K smpl:666K ep:2K epch:55.44 loss:0.008 grdn:0.174 lr:7.4e-05 updt_s:0.330 data_s:0.041\n",
      "WARNING 2025-12-05 18:57:27 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:57:27 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:58:42 ot_train.py:351 step:11K smpl:678K ep:2K epch:56.51 loss:0.008 grdn:0.180 lr:7.3e-05 updt_s:0.332 data_s:0.041\n",
      "WARNING 2025-12-05 18:58:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:58:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 18:59:56 ot_train.py:351 step:11K smpl:691K ep:2K epch:57.58 loss:0.008 grdn:0.168 lr:7.2e-05 updt_s:0.330 data_s:0.042\n",
      "WARNING 2025-12-05 18:59:56 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 18:59:56 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:01:11 ot_train.py:351 step:11K smpl:704K ep:2K epch:58.64 loss:0.008 grdn:0.168 lr:7.2e-05 updt_s:0.331 data_s:0.040\n",
      "WARNING 2025-12-05 19:01:11 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:01:11 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:02:26 ot_train.py:351 step:11K smpl:717K ep:2K epch:59.71 loss:0.008 grdn:0.170 lr:7.1e-05 updt_s:0.331 data_s:0.046\n",
      "WARNING 2025-12-05 19:02:26 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:02:26 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:03:41 ot_train.py:351 step:11K smpl:730K ep:2K epch:60.77 loss:0.007 grdn:0.156 lr:7.0e-05 updt_s:0.328 data_s:0.045\n",
      "WARNING 2025-12-05 19:03:41 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:03:41 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:04:55 ot_train.py:351 step:12K smpl:742K ep:2K epch:61.84 loss:0.007 grdn:0.164 lr:6.9e-05 updt_s:0.329 data_s:0.039\n",
      "WARNING 2025-12-05 19:04:55 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:04:55 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:06:10 ot_train.py:351 step:12K smpl:755K ep:2K epch:62.91 loss:0.007 grdn:0.161 lr:6.8e-05 updt_s:0.328 data_s:0.043\n",
      "WARNING 2025-12-05 19:06:10 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:06:10 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:07:24 ot_train.py:351 step:12K smpl:768K ep:2K epch:63.97 loss:0.007 grdn:0.161 lr:6.7e-05 updt_s:0.329 data_s:0.042\n",
      "WARNING 2025-12-05 19:07:24 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:07:24 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:08:38 ot_train.py:351 step:12K smpl:781K ep:2K epch:65.04 loss:0.007 grdn:0.153 lr:6.6e-05 updt_s:0.330 data_s:0.039\n",
      "WARNING 2025-12-05 19:08:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:08:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:09:53 ot_train.py:351 step:12K smpl:794K ep:2K epch:66.11 loss:0.007 grdn:0.156 lr:6.5e-05 updt_s:0.329 data_s:0.044\n",
      "WARNING 2025-12-05 19:09:53 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:09:53 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:11:14 ot_train.py:351 step:13K smpl:806K ep:2K epch:67.17 loss:0.007 grdn:0.153 lr:6.4e-05 updt_s:0.330 data_s:0.074\n",
      "WARNING 2025-12-05 19:11:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:11:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:12:29 ot_train.py:351 step:13K smpl:819K ep:2K epch:68.24 loss:0.007 grdn:0.154 lr:6.3e-05 updt_s:0.329 data_s:0.041\n",
      "WARNING 2025-12-05 19:12:29 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:12:29 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:13:45 ot_train.py:351 step:13K smpl:832K ep:2K epch:69.30 loss:0.007 grdn:0.153 lr:6.2e-05 updt_s:0.331 data_s:0.046\n",
      "WARNING 2025-12-05 19:13:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:13:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:14:59 ot_train.py:351 step:13K smpl:845K ep:2K epch:70.37 loss:0.006 grdn:0.151 lr:6.1e-05 updt_s:0.328 data_s:0.042\n",
      "WARNING 2025-12-05 19:14:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:14:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:16:13 ot_train.py:351 step:13K smpl:858K ep:2K epch:71.44 loss:0.006 grdn:0.147 lr:6.0e-05 updt_s:0.328 data_s:0.042\n",
      "WARNING 2025-12-05 19:16:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:16:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:17:27 ot_train.py:351 step:14K smpl:870K ep:2K epch:72.50 loss:0.006 grdn:0.147 lr:5.9e-05 updt_s:0.331 data_s:0.041\n",
      "WARNING 2025-12-05 19:17:27 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:17:27 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:18:43 ot_train.py:351 step:14K smpl:883K ep:2K epch:73.57 loss:0.006 grdn:0.150 lr:5.8e-05 updt_s:0.333 data_s:0.043\n",
      "WARNING 2025-12-05 19:18:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:18:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:19:58 ot_train.py:351 step:14K smpl:896K ep:2K epch:74.64 loss:0.006 grdn:0.140 lr:5.7e-05 updt_s:0.330 data_s:0.043\n",
      "WARNING 2025-12-05 19:19:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:19:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:21:13 ot_train.py:351 step:14K smpl:909K ep:2K epch:75.70 loss:0.006 grdn:0.144 lr:5.6e-05 updt_s:0.332 data_s:0.043\n",
      "WARNING 2025-12-05 19:21:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:21:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:22:29 ot_train.py:351 step:14K smpl:922K ep:2K epch:76.77 loss:0.006 grdn:0.142 lr:5.5e-05 updt_s:0.329 data_s:0.048\n",
      "WARNING 2025-12-05 19:22:29 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:22:29 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:23:43 ot_train.py:351 step:15K smpl:934K ep:2K epch:77.83 loss:0.006 grdn:0.142 lr:5.4e-05 updt_s:0.330 data_s:0.042\n",
      "WARNING 2025-12-05 19:23:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:23:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:24:58 ot_train.py:351 step:15K smpl:947K ep:2K epch:78.90 loss:0.006 grdn:0.146 lr:5.3e-05 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 19:24:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:24:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:26:14 ot_train.py:351 step:15K smpl:960K ep:2K epch:79.97 loss:0.005 grdn:0.138 lr:5.2e-05 updt_s:0.331 data_s:0.043\n",
      "WARNING 2025-12-05 19:26:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:26:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-05 19:26:14 ot_train.py:361 Checkpoint policy after step 15000\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:27:32 ot_train.py:351 step:15K smpl:973K ep:2K epch:81.03 loss:0.005 grdn:0.133 lr:5.1e-05 updt_s:0.326 data_s:0.047\n",
      "WARNING 2025-12-05 19:27:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:27:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:28:46 ot_train.py:351 step:15K smpl:986K ep:2K epch:82.10 loss:0.005 grdn:0.137 lr:5.0e-05 updt_s:0.330 data_s:0.039\n",
      "WARNING 2025-12-05 19:28:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:28:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:30:01 ot_train.py:351 step:16K smpl:998K ep:2K epch:83.17 loss:0.005 grdn:0.138 lr:4.9e-05 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 19:30:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:30:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:31:21 ot_train.py:351 step:16K smpl:1M ep:3K epch:84.23 loss:0.005 grdn:0.136 lr:4.8e-05 updt_s:0.329 data_s:0.070\n",
      "WARNING 2025-12-05 19:31:21 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:31:21 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:32:35 ot_train.py:351 step:16K smpl:1M ep:3K epch:85.30 loss:0.005 grdn:0.130 lr:4.7e-05 updt_s:0.331 data_s:0.041\n",
      "WARNING 2025-12-05 19:32:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:32:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:33:49 ot_train.py:351 step:16K smpl:1M ep:3K epch:86.36 loss:0.005 grdn:0.132 lr:4.6e-05 updt_s:0.329 data_s:0.041\n",
      "WARNING 2025-12-05 19:33:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:33:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:35:04 ot_train.py:351 step:16K smpl:1M ep:3K epch:87.43 loss:0.005 grdn:0.127 lr:4.5e-05 updt_s:0.330 data_s:0.040\n",
      "WARNING 2025-12-05 19:35:04 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:35:04 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:36:18 ot_train.py:351 step:17K smpl:1M ep:3K epch:88.50 loss:0.005 grdn:0.125 lr:4.4e-05 updt_s:0.329 data_s:0.042\n",
      "WARNING 2025-12-05 19:36:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:36:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:37:33 ot_train.py:351 step:17K smpl:1M ep:3K epch:89.56 loss:0.005 grdn:0.128 lr:4.3e-05 updt_s:0.330 data_s:0.042\n",
      "WARNING 2025-12-05 19:37:33 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:37:33 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:38:48 ot_train.py:351 step:17K smpl:1M ep:3K epch:90.63 loss:0.005 grdn:0.127 lr:4.2e-05 updt_s:0.330 data_s:0.044\n",
      "WARNING 2025-12-05 19:38:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:38:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:40:03 ot_train.py:351 step:17K smpl:1M ep:3K epch:91.70 loss:0.005 grdn:0.127 lr:4.1e-05 updt_s:0.330 data_s:0.041\n",
      "WARNING 2025-12-05 19:40:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:40:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:41:17 ot_train.py:351 step:17K smpl:1M ep:3K epch:92.76 loss:0.004 grdn:0.124 lr:4.0e-05 updt_s:0.329 data_s:0.042\n",
      "WARNING 2025-12-05 19:41:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:41:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:42:32 ot_train.py:351 step:18K smpl:1M ep:3K epch:93.83 loss:0.004 grdn:0.123 lr:3.9e-05 updt_s:0.329 data_s:0.043\n",
      "WARNING 2025-12-05 19:42:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:42:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:43:47 ot_train.py:351 step:18K smpl:1M ep:3K epch:94.89 loss:0.005 grdn:0.122 lr:3.8e-05 updt_s:0.330 data_s:0.044\n",
      "WARNING 2025-12-05 19:43:47 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:43:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:45:02 ot_train.py:351 step:18K smpl:1M ep:3K epch:95.96 loss:0.004 grdn:0.115 lr:3.7e-05 updt_s:0.329 data_s:0.043\n",
      "WARNING 2025-12-05 19:45:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:45:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:46:17 ot_train.py:351 step:18K smpl:1M ep:3K epch:97.03 loss:0.004 grdn:0.116 lr:3.6e-05 updt_s:0.330 data_s:0.043\n",
      "WARNING 2025-12-05 19:46:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:46:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:47:32 ot_train.py:351 step:18K smpl:1M ep:3K epch:98.09 loss:0.004 grdn:0.120 lr:3.5e-05 updt_s:0.332 data_s:0.043\n",
      "WARNING 2025-12-05 19:47:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:47:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:48:47 ot_train.py:351 step:19K smpl:1M ep:3K epch:99.16 loss:0.004 grdn:0.111 lr:3.4e-05 updt_s:0.327 data_s:0.047\n",
      "WARNING 2025-12-05 19:48:47 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:48:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:50:02 ot_train.py:351 step:19K smpl:1M ep:3K epch:100.22 loss:0.004 grdn:0.112 lr:3.3e-05 updt_s:0.329 data_s:0.044\n",
      "WARNING 2025-12-05 19:50:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:50:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:51:23 ot_train.py:351 step:19K smpl:1M ep:3K epch:101.29 loss:0.004 grdn:0.112 lr:3.2e-05 updt_s:0.331 data_s:0.073\n",
      "WARNING 2025-12-05 19:51:23 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:51:23 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:52:38 ot_train.py:351 step:19K smpl:1M ep:3K epch:102.36 loss:0.004 grdn:0.107 lr:3.1e-05 updt_s:0.329 data_s:0.042\n",
      "WARNING 2025-12-05 19:52:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:52:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:53:54 ot_train.py:351 step:19K smpl:1M ep:3K epch:103.42 loss:0.004 grdn:0.114 lr:3.0e-05 updt_s:0.332 data_s:0.045\n",
      "WARNING 2025-12-05 19:53:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:53:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:55:09 ot_train.py:351 step:20K smpl:1M ep:3K epch:104.49 loss:0.004 grdn:0.111 lr:2.9e-05 updt_s:0.331 data_s:0.045\n",
      "WARNING 2025-12-05 19:55:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:55:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:56:25 ot_train.py:351 step:20K smpl:1M ep:3K epch:105.56 loss:0.004 grdn:0.106 lr:2.8e-05 updt_s:0.332 data_s:0.045\n",
      "WARNING 2025-12-05 19:56:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:56:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:57:40 ot_train.py:351 step:20K smpl:1M ep:3K epch:106.62 loss:0.004 grdn:0.104 lr:2.7e-05 updt_s:0.332 data_s:0.041\n",
      "WARNING 2025-12-05 19:57:40 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:57:40 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-05 19:57:40 ot_train.py:361 Checkpoint policy after step 20000\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 19:58:58 ot_train.py:351 step:20K smpl:1M ep:3K epch:107.69 loss:0.004 grdn:0.105 lr:2.6e-05 updt_s:0.329 data_s:0.043\n",
      "WARNING 2025-12-05 19:58:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 19:58:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:00:13 ot_train.py:351 step:20K smpl:1M ep:3K epch:108.75 loss:0.004 grdn:0.103 lr:2.6e-05 updt_s:0.330 data_s:0.042\n",
      "WARNING 2025-12-05 20:00:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:00:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:01:28 ot_train.py:351 step:21K smpl:1M ep:3K epch:109.82 loss:0.004 grdn:0.104 lr:2.5e-05 updt_s:0.333 data_s:0.045\n",
      "WARNING 2025-12-05 20:01:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:01:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:02:44 ot_train.py:351 step:21K smpl:1M ep:3K epch:110.89 loss:0.004 grdn:0.098 lr:2.4e-05 updt_s:0.332 data_s:0.045\n",
      "WARNING 2025-12-05 20:02:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:02:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:04:00 ot_train.py:351 step:21K smpl:1M ep:3K epch:111.95 loss:0.004 grdn:0.102 lr:2.3e-05 updt_s:0.331 data_s:0.045\n",
      "WARNING 2025-12-05 20:04:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:04:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:05:15 ot_train.py:351 step:21K smpl:1M ep:3K epch:113.02 loss:0.003 grdn:0.093 lr:2.2e-05 updt_s:0.331 data_s:0.041\n",
      "WARNING 2025-12-05 20:05:15 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:05:15 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:06:29 ot_train.py:351 step:21K smpl:1M ep:3K epch:114.09 loss:0.003 grdn:0.095 lr:2.1e-05 updt_s:0.330 data_s:0.043\n",
      "WARNING 2025-12-05 20:06:29 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:06:29 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:07:44 ot_train.py:351 step:22K smpl:1M ep:3K epch:115.15 loss:0.003 grdn:0.096 lr:2.1e-05 updt_s:0.332 data_s:0.042\n",
      "WARNING 2025-12-05 20:07:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:07:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:08:58 ot_train.py:351 step:22K smpl:1M ep:3K epch:116.22 loss:0.004 grdn:0.098 lr:2.0e-05 updt_s:0.329 data_s:0.038\n",
      "WARNING 2025-12-05 20:08:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:08:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:10:18 ot_train.py:351 step:22K smpl:1M ep:4K epch:117.28 loss:0.003 grdn:0.094 lr:1.9e-05 updt_s:0.329 data_s:0.068\n",
      "WARNING 2025-12-05 20:10:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:10:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:11:32 ot_train.py:351 step:22K smpl:1M ep:4K epch:118.35 loss:0.004 grdn:0.098 lr:1.8e-05 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 20:11:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:11:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:12:46 ot_train.py:351 step:22K smpl:1M ep:4K epch:119.42 loss:0.003 grdn:0.090 lr:1.8e-05 updt_s:0.328 data_s:0.041\n",
      "WARNING 2025-12-05 20:12:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:12:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:14:01 ot_train.py:351 step:23K smpl:1M ep:4K epch:120.48 loss:0.003 grdn:0.093 lr:1.7e-05 updt_s:0.329 data_s:0.044\n",
      "WARNING 2025-12-05 20:14:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:14:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:15:16 ot_train.py:351 step:23K smpl:1M ep:4K epch:121.55 loss:0.003 grdn:0.091 lr:1.6e-05 updt_s:0.329 data_s:0.042\n",
      "WARNING 2025-12-05 20:15:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:15:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:16:30 ot_train.py:351 step:23K smpl:1M ep:4K epch:122.62 loss:0.003 grdn:0.087 lr:1.5e-05 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 20:16:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:16:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:17:45 ot_train.py:351 step:23K smpl:1M ep:4K epch:123.68 loss:0.003 grdn:0.091 lr:1.5e-05 updt_s:0.331 data_s:0.041\n",
      "WARNING 2025-12-05 20:17:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:17:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:19:00 ot_train.py:351 step:23K smpl:1M ep:4K epch:124.75 loss:0.003 grdn:0.089 lr:1.4e-05 updt_s:0.331 data_s:0.041\n",
      "WARNING 2025-12-05 20:19:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:19:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:20:14 ot_train.py:351 step:24K smpl:2M ep:4K epch:125.81 loss:0.003 grdn:0.089 lr:1.3e-05 updt_s:0.332 data_s:0.040\n",
      "WARNING 2025-12-05 20:20:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:20:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:21:30 ot_train.py:351 step:24K smpl:2M ep:4K epch:126.88 loss:0.003 grdn:0.082 lr:1.3e-05 updt_s:0.330 data_s:0.047\n",
      "WARNING 2025-12-05 20:21:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:21:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:22:44 ot_train.py:351 step:24K smpl:2M ep:4K epch:127.95 loss:0.003 grdn:0.086 lr:1.2e-05 updt_s:0.330 data_s:0.042\n",
      "WARNING 2025-12-05 20:22:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:22:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:23:59 ot_train.py:351 step:24K smpl:2M ep:4K epch:129.01 loss:0.003 grdn:0.081 lr:1.2e-05 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 20:23:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:23:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:25:14 ot_train.py:351 step:24K smpl:2M ep:4K epch:130.08 loss:0.003 grdn:0.080 lr:1.1e-05 updt_s:0.330 data_s:0.040\n",
      "WARNING 2025-12-05 20:25:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:25:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:26:30 ot_train.py:351 step:25K smpl:2M ep:4K epch:131.15 loss:0.003 grdn:0.077 lr:1.0e-05 updt_s:0.330 data_s:0.049\n",
      "WARNING 2025-12-05 20:26:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:26:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:27:45 ot_train.py:351 step:25K smpl:2M ep:4K epch:132.21 loss:0.003 grdn:0.078 lr:9.8e-06 updt_s:0.332 data_s:0.040\n",
      "WARNING 2025-12-05 20:27:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:27:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:28:59 ot_train.py:351 step:25K smpl:2M ep:4K epch:133.28 loss:0.003 grdn:0.077 lr:9.3e-06 updt_s:0.329 data_s:0.042\n",
      "WARNING 2025-12-05 20:28:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:28:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-05 20:28:59 ot_train.py:361 Checkpoint policy after step 25000\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:30:22 ot_train.py:351 step:25K smpl:2M ep:4K epch:134.34 loss:0.003 grdn:0.078 lr:8.8e-06 updt_s:0.331 data_s:0.067\n",
      "WARNING 2025-12-05 20:30:22 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:30:22 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:31:38 ot_train.py:351 step:25K smpl:2M ep:4K epch:135.41 loss:0.003 grdn:0.075 lr:8.3e-06 updt_s:0.332 data_s:0.046\n",
      "WARNING 2025-12-05 20:31:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:31:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:32:53 ot_train.py:351 step:26K smpl:2M ep:4K epch:136.48 loss:0.003 grdn:0.077 lr:7.8e-06 updt_s:0.330 data_s:0.042\n",
      "WARNING 2025-12-05 20:32:53 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:32:53 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:34:07 ot_train.py:351 step:26K smpl:2M ep:4K epch:137.54 loss:0.003 grdn:0.079 lr:7.4e-06 updt_s:0.331 data_s:0.039\n",
      "WARNING 2025-12-05 20:34:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:34:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:35:22 ot_train.py:351 step:26K smpl:2M ep:4K epch:138.61 loss:0.003 grdn:0.075 lr:6.9e-06 updt_s:0.331 data_s:0.043\n",
      "WARNING 2025-12-05 20:35:22 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:35:22 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:36:37 ot_train.py:351 step:26K smpl:2M ep:4K epch:139.68 loss:0.003 grdn:0.073 lr:6.5e-06 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 20:36:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:36:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:37:52 ot_train.py:351 step:26K smpl:2M ep:4K epch:140.74 loss:0.003 grdn:0.074 lr:6.1e-06 updt_s:0.331 data_s:0.043\n",
      "WARNING 2025-12-05 20:37:52 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:37:52 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:39:07 ot_train.py:351 step:27K smpl:2M ep:4K epch:141.81 loss:0.003 grdn:0.073 lr:5.7e-06 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 20:39:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:39:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:40:22 ot_train.py:351 step:27K smpl:2M ep:4K epch:142.87 loss:0.003 grdn:0.072 lr:5.4e-06 updt_s:0.332 data_s:0.042\n",
      "WARNING 2025-12-05 20:40:22 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:40:22 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:41:37 ot_train.py:351 step:27K smpl:2M ep:4K epch:143.94 loss:0.003 grdn:0.069 lr:5.0e-06 updt_s:0.333 data_s:0.042\n",
      "WARNING 2025-12-05 20:41:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:41:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:42:53 ot_train.py:351 step:27K smpl:2M ep:4K epch:145.01 loss:0.003 grdn:0.071 lr:4.7e-06 updt_s:0.332 data_s:0.042\n",
      "WARNING 2025-12-05 20:42:53 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:42:53 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:44:08 ot_train.py:351 step:27K smpl:2M ep:4K epch:146.07 loss:0.003 grdn:0.069 lr:4.4e-06 updt_s:0.332 data_s:0.043\n",
      "WARNING 2025-12-05 20:44:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:44:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:45:23 ot_train.py:351 step:28K smpl:2M ep:4K epch:147.14 loss:0.003 grdn:0.069 lr:4.2e-06 updt_s:0.332 data_s:0.043\n",
      "WARNING 2025-12-05 20:45:23 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:45:23 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:46:39 ot_train.py:351 step:28K smpl:2M ep:4K epch:148.20 loss:0.003 grdn:0.072 lr:3.9e-06 updt_s:0.330 data_s:0.044\n",
      "WARNING 2025-12-05 20:46:39 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:46:39 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:47:54 ot_train.py:351 step:28K smpl:2M ep:4K epch:149.27 loss:0.003 grdn:0.068 lr:3.7e-06 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 20:47:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:47:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:49:10 ot_train.py:351 step:28K smpl:2M ep:5K epch:150.34 loss:0.003 grdn:0.067 lr:3.5e-06 updt_s:0.333 data_s:0.046\n",
      "WARNING 2025-12-05 20:49:10 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:49:10 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:50:31 ot_train.py:351 step:28K smpl:2M ep:5K epch:151.40 loss:0.003 grdn:0.069 lr:3.3e-06 updt_s:0.334 data_s:0.071\n",
      "WARNING 2025-12-05 20:50:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:50:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:51:47 ot_train.py:351 step:29K smpl:2M ep:5K epch:152.47 loss:0.003 grdn:0.067 lr:3.1e-06 updt_s:0.332 data_s:0.046\n",
      "WARNING 2025-12-05 20:51:47 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:51:47 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:53:02 ot_train.py:351 step:29K smpl:2M ep:5K epch:153.54 loss:0.003 grdn:0.069 lr:3.0e-06 updt_s:0.331 data_s:0.040\n",
      "WARNING 2025-12-05 20:53:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:53:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:54:16 ot_train.py:351 step:29K smpl:2M ep:5K epch:154.60 loss:0.003 grdn:0.068 lr:2.8e-06 updt_s:0.330 data_s:0.041\n",
      "WARNING 2025-12-05 20:54:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:54:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:55:32 ot_train.py:351 step:29K smpl:2M ep:5K epch:155.67 loss:0.003 grdn:0.068 lr:2.7e-06 updt_s:0.334 data_s:0.043\n",
      "WARNING 2025-12-05 20:55:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:55:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:56:48 ot_train.py:351 step:29K smpl:2M ep:5K epch:156.73 loss:0.003 grdn:0.067 lr:2.6e-06 updt_s:0.333 data_s:0.044\n",
      "WARNING 2025-12-05 20:56:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:56:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:58:03 ot_train.py:351 step:30K smpl:2M ep:5K epch:157.80 loss:0.003 grdn:0.065 lr:2.6e-06 updt_s:0.331 data_s:0.042\n",
      "WARNING 2025-12-05 20:58:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:58:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 20:59:17 ot_train.py:351 step:30K smpl:2M ep:5K epch:158.87 loss:0.003 grdn:0.065 lr:2.5e-06 updt_s:0.329 data_s:0.040\n",
      "WARNING 2025-12-05 20:59:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 20:59:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO 2025-12-05 21:00:31 ot_train.py:351 step:30K smpl:2M ep:5K epch:159.93 loss:0.003 grdn:0.065 lr:2.5e-06 updt_s:0.327 data_s:0.042\n",
      "WARNING 2025-12-05 21:00:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "WARNING 2025-12-05 21:00:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
      "INFO 2025-12-05 21:00:31 ot_train.py:361 Checkpoint policy after step 30000\n",
      "INFO 2025-12-05 21:00:34 ot_train.py:430 End of training\n"
     ]
    }
   ],
   "source": [
    "!lerobot-train \\\n",
    "  --dataset.repo_id=nkmurst/amd_record-test3 \\\n",
    "  --batch_size=64 \\\n",
    "  --steps=30000 \\\n",
    "  --output_dir=outputs/train/so101_petcap_smolvla2 \\\n",
    "  --job_name=so101_petcap_smolvla2 \\\n",
    "  --policy.device=cuda \\\n",
    "  --policy.path=lerobot/smolvla_base \\\n",
    "  --policy.push_to_hub=false \\\n",
    "  --wandb.enable=true \\\n",
    "  --save_freq=5000 \\\n",
    "  --rename_map='{\"observation.images.top\": \"observation.images.camera1\", \"observation.images.side\": \"observation.images.camera2\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "- If using a local dataset, add `--dataset.root=/path/to/dataset`.\n",
    "- Adjust `--batch_size` and `--steps` based on your hardware and dataset.\n",
    "- Model checkpoints, logs, and training plots will be saved to the specified `--output_dir`\n",
    "- Training progress visualized in your wandb dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Checkpoints to Hugging Face\n",
    "Now after training is done, upload the last checkpoint. You may refer to [here](https://github.com/huggingface/lerobot/blob/v0.4.0/README.md#add-a-pretrained-policy) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFMLGuVkH7UN",
    "outputId": "535f0717-4d6b-4eeb-beee-25416f7af383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
      "Start hashing 7 files.\n",
      "Finished hashing 7 files.\n",
      "Processing Files (0 / 0)      : |                  |  0.00B /  0.00B            \n",
      "New Data Upload               : |                  |  0.00B /  0.00B            \u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  ...d_model/model.safetensors:  13%|█▊            |  117MB /  907MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  13%|█▊            |  117MB /  907MB,   ???B/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  27%|███▊          |  243MB /  907MB,  629MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  41%|█████▋        |  369MB /  907MB,  629MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  54%|███████▌      |  486MB /  907MB,  615MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  68%|█████████▍    |  612MB /  907MB,  619MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  81%|███████████▍  |  738MB /  907MB,  621MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (2 / 3)      :  94%|█████████████▏|  856MB /  907MB,  615MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (3 / 3)      : 100%|██████████████|  907MB /  907MB,  564MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  ...d_model/model.safetensors: 100%|██████████████|  907MB /  907MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  ...d_model/model.safetensors: 100%|██████████████|  907MB /  907MB            \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing Files (3 / 3)      : 100%|██████████████|  907MB /  907MB,  439MB/s  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "New Data Upload               : |                  |  0.00B /  0.00B,  0.00B/s  \n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \n",
      "  ...zer_processor.safetensors: 100%|██████████████| 7.52kB / 7.52kB            \n",
      "  ...d_model/model.safetensors: 100%|██████████████|  907MB /  907MB            \n",
      "https://huggingface.co/mimi-neoki/policy_so101_petcap_smolvla2-10000/tree/main/.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli upload mimi-neoki/policy_so101_petcap_smolvla2-10000 outputs/train/so101_petcap_smolvla2/checkpoints/010000/pretrained_model\n",
    "# e.g. huggingface-cli upload ${HF_USER}/act_so101_3cube_1ksteps \\\n",
    "#  outputs/train/act_so101_3cube_1ksteps/checkpoints/last/pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscs\n",
    "1. Once the environment is setup, you can open a terminal session for training by navigating to `File → New Launcher → Other → Terminal`.\n",
    "2. You can also upload your datasets to the container by clicking the `Upload Files` button in the left pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "1. If you encounter an error like:\n",
    "   ```\n",
    "   FileExistsError: Output directory outputs/train/act_so101_3cube_1ksteps already exists and resume is False. Please change your output directory so that outputs/train/act_so101_3cube_1ksteps is not overwritten. \n",
    "   ```\n",
    "   Remove the existing directory before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFMLGuVkH7UN",
    "outputId": "535f0717-4d6b-4eeb-beee-25416f7af383"
   },
   "outputs": [],
   "source": [
    "!rm -fr outputs/train/act_so101_3cube_1ksteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When running models other than ACT, ensure you install the required additional dependencies for those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/lerobot\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets<4.2.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.1)\n",
      "Requirement already satisfied: diffusers<0.36.0,>=0.27.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.35.2)\n",
      "Requirement already satisfied: huggingface-hub<0.36.0,>=0.34.2 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.35.3)\n",
      "Requirement already satisfied: accelerate<2.0.0,>=1.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.12.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=71.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (80.9.0)\n",
      "Requirement already satisfied: cmake<4.2.0,>=3.29.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.3)\n",
      "Requirement already satisfied: einops<0.9.0,>=0.8.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.8.1)\n",
      "Requirement already satisfied: opencv-python-headless<4.13.0,>=4.9.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.12.0.88)\n",
      "Requirement already satisfied: av<16.0.0,>=15.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (15.1.0)\n",
      "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.0.0)\n",
      "Requirement already satisfied: packaging<26.0,>=24.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (25.0)\n",
      "Requirement already satisfied: pynput<1.9.0,>=1.7.7 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.8.1)\n",
      "Requirement already satisfied: pyserial<4.0,>=3.5 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: wandb<0.22.0,>=0.20.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.21.4)\n",
      "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (2.7.1+rocm7.0.0.lw.git698b58a9)\n",
      "Requirement already satisfied: torchcodec<0.6.0,>=0.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.5)\n",
      "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.22.1+rocm7.0.0.git59a3e1f9)\n",
      "Requirement already satisfied: draccus==0.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.10.0)\n",
      "Requirement already satisfied: gymnasium<2.0.0,>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.2.2)\n",
      "Requirement already satisfied: rerun-sdk<0.27.0,>=0.24.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.26.2)\n",
      "Requirement already satisfied: deepdiff<9.0.0,>=7.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (8.6.1)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.34.0 in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (2.37.2)\n",
      "Requirement already satisfied: termcolor<4.0.0,>=2.4.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.2.0)\n",
      "Collecting num2words<0.6.0,>=0.5.14 (from lerobot==0.4.1)\n",
      "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: safetensors<1.0.0,>=0.4.3 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: mergedeep~=1.3 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.3.4)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (6.0.2)\n",
      "Requirement already satisfied: pyyaml-include~=1.4 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.4.1)\n",
      "Requirement already satisfied: toml~=0.10 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.10.2)\n",
      "Requirement already satisfied: typing-inspect~=0.9.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (2.2.6)\n",
      "Requirement already satisfied: psutil in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (7.1.3)\n",
      "Requirement already satisfied: filelock in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.9.0)\n",
      "Requirement already satisfied: orderly-set<6,>=5.4.1 in /opt/venv/lib/python3.12/site-packages (from deepdiff<9.0.0,>=7.0.1->lerobot==0.4.1) (5.5.0)\n",
      "Requirement already satisfied: importlib_metadata in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (8.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (2025.11.3)\n",
      "Requirement already satisfied: Pillow in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (11.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.13.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (0.0.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (1.2.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.1.9)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (3.0.52)\n",
      "Requirement already satisfied: imageio-ffmpeg in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (0.6.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/venv/lib/python3.12/site-packages (from jsonlines<5.0.0,>=4.0.0->lerobot==0.4.1) (25.4.0)\n",
      "Collecting docopt>=0.6.2 (from num2words<0.6.0,>=0.5.14->lerobot==0.4.1)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /opt/venv/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.2.14)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.17.0)\n",
      "Requirement already satisfied: evdev>=1.3 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.9.2)\n",
      "Requirement already satisfied: python-xlib>=0.17 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (0.33)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.1.6)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.3.1+rocm7.0.0.git9c7bc0a3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.3.1+rocm7.0.0.git9c7bc0a3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.4.1) (1.1.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (6.33.1)\n",
      "Requirement already satisfied: pydantic<3 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.12.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.47.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/venv/lib/python3.12/site-packages (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.12/site-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.0.2)\n",
      "Collecting transformers<5.0.0,>=4.53.0 (from lerobot==0.4.1)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.53.0->lerobot==0.4.1)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lerobot, docopt\n",
      "  Building editable for lerobot (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lerobot: filename=lerobot-0.4.1-0.editable-py3-none-any.whl size=15631 sha256=31738346e564cd053109a1af0752bc0c7056748f8c9826d5c6c8bb4731c88854\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qgueg0eb/wheels/05/0a/0d/80a4c08845345c44fe1e5f70929884983b90d85f46a77f7601\n",
      "  Building wheel for docopt (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=7662466a1a3e23858a35006a1b25b288f482219271ab0efe163157f030c41fe7\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built lerobot docopt\n",
      "Installing collected packages: docopt, num2words, tokenizers, transformers, lerobot\n",
      "\u001b[2K  Attempting uninstall: lerobot━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: lerobot 0.4.1m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling lerobot-0.4.1:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled lerobot-0.4.1\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [lerobot]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [lerobot]\u001b[0m [lerobot]\n",
      "\u001b[1A\u001b[2KSuccessfully installed docopt-0.6.2 lerobot-0.4.1 num2words-0.5.14 tokenizers-0.22.1 transformers-4.57.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Obtaining file:///workspace/lerobot\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets<4.2.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.1)\n",
      "Requirement already satisfied: diffusers<0.36.0,>=0.27.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.35.2)\n",
      "Requirement already satisfied: huggingface-hub<0.36.0,>=0.34.2 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.35.3)\n",
      "Requirement already satisfied: accelerate<2.0.0,>=1.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.12.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=71.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (80.9.0)\n",
      "Requirement already satisfied: cmake<4.2.0,>=3.29.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.1.3)\n",
      "Requirement already satisfied: einops<0.9.0,>=0.8.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.8.1)\n",
      "Requirement already satisfied: opencv-python-headless<4.13.0,>=4.9.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.12.0.88)\n",
      "Requirement already satisfied: av<16.0.0,>=15.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (15.1.0)\n",
      "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (4.0.0)\n",
      "Requirement already satisfied: packaging<26.0,>=24.2 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (25.0)\n",
      "Requirement already satisfied: pynput<1.9.0,>=1.7.7 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.8.1)\n",
      "Requirement already satisfied: pyserial<4.0,>=3.5 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: wandb<0.22.0,>=0.20.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.21.4)\n",
      "Requirement already satisfied: torch<2.8.0,>=2.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (2.7.1+rocm7.0.0.lw.git698b58a9)\n",
      "Requirement already satisfied: torchcodec<0.6.0,>=0.2.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.5)\n",
      "Requirement already satisfied: torchvision<0.23.0,>=0.21.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.22.1+rocm7.0.0.git59a3e1f9)\n",
      "Requirement already satisfied: draccus==0.10.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.10.0)\n",
      "Requirement already satisfied: gymnasium<2.0.0,>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (1.2.2)\n",
      "Requirement already satisfied: rerun-sdk<0.27.0,>=0.24.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (0.26.2)\n",
      "Requirement already satisfied: deepdiff<9.0.0,>=7.0.1 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (8.6.1)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.34.0 in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (2.37.2)\n",
      "Requirement already satisfied: termcolor<4.0.0,>=2.4.0 in /opt/venv/lib/python3.12/site-packages (from lerobot==0.4.1) (3.2.0)\n",
      "Collecting transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi (from lerobot==0.4.1)\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision fix/lerobot_openpi) to /tmp/pip-install-3a152ygj/transformers_96fdb218782745f7b66dc1f3f6fe4935\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-3a152ygj/transformers_96fdb218782745f7b66dc1f3f6fe4935\n",
      "  Running command git checkout -b fix/lerobot_openpi --track origin/fix/lerobot_openpi\n",
      "  Switched to a new branch 'fix/lerobot_openpi'\n",
      "  branch 'fix/lerobot_openpi' set up to track 'origin/fix/lerobot_openpi'.\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit dcddb970176382c0fcf4521b0c0e6fc15894dfe0\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/venv/lib/python3.12/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/venv/lib/python3.12/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (2.2.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/venv/lib/python3.12/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/venv/lib/python3.12/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/venv/lib/python3.12/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/venv/lib/python3.12/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/venv/lib/python3.12/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (4.67.1)\n",
      "Requirement already satisfied: mergedeep~=1.3 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.3.4)\n",
      "Requirement already satisfied: pyyaml-include~=1.4 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (1.4.1)\n",
      "Requirement already satisfied: toml~=0.10 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.10.2)\n",
      "Requirement already satisfied: typing-inspect~=0.9.0 in /opt/venv/lib/python3.12/site-packages (from draccus==0.10.0->lerobot==0.4.1) (0.9.0)\n",
      "Requirement already satisfied: psutil in /opt/venv/lib/python3.12/site-packages (from accelerate<2.0.0,>=1.10.0->lerobot==0.4.1) (7.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/venv/lib/python3.12/site-packages (from datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.9.0)\n",
      "Requirement already satisfied: orderly-set<6,>=5.4.1 in /opt/venv/lib/python3.12/site-packages (from deepdiff<9.0.0,>=7.0.1->lerobot==0.4.1) (5.5.0)\n",
      "Requirement already satisfied: importlib_metadata in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (8.7.0)\n",
      "Requirement already satisfied: Pillow in /opt/venv/lib/python3.12/site-packages (from diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (11.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (3.13.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/venv/lib/python3.12/site-packages (from gymnasium<2.0.0,>=1.1.1->lerobot==0.4.1) (0.0.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub<0.36.0,>=0.34.2->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (1.2.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /opt/venv/lib/python3.12/site-packages (from huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.1.9)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (3.0.52)\n",
      "Requirement already satisfied: imageio-ffmpeg in /opt/venv/lib/python3.12/site-packages (from imageio[ffmpeg]<3.0.0,>=2.34.0->lerobot==0.4.1) (0.6.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/venv/lib/python3.12/site-packages (from jsonlines<5.0.0,>=4.0.0->lerobot==0.4.1) (25.4.0)\n",
      "Requirement already satisfied: wcwidth in /opt/venv/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]<0.36.0,>=0.34.2->lerobot==0.4.1) (0.2.14)\n",
      "Requirement already satisfied: six in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.17.0)\n",
      "Requirement already satisfied: evdev>=1.3 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (1.9.2)\n",
      "Requirement already satisfied: python-xlib>=0.17 in /opt/venv/lib/python3.12/site-packages (from pynput<1.9.0,>=1.7.7->lerobot==0.4.1) (0.33)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.1.6)\n",
      "Requirement already satisfied: pytorch-triton-rocm==3.3.1+rocm7.0.0.git9c7bc0a3 in /opt/venv/lib/python3.12/site-packages (from torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.3.1+rocm7.0.0.git9c7bc0a3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/venv/lib/python3.12/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.4.1) (1.1.0)\n",
      "Requirement already satisfied: click>=8.0.1 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.5.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (6.33.1)\n",
      "Requirement already satisfied: pydantic<3 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.12.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/venv/lib/python3.12/site-packages (from wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.47.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/venv/lib/python3.12/site-packages (from pydantic<3->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venv/lib/python3.12/site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.12/site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.12/site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.12/site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi->lerobot==0.4.1) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.22.0,>=0.20.0->lerobot==0.4.1) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/venv/lib/python3.12/site-packages (from importlib_metadata->diffusers<0.36.0,>=0.27.2->lerobot==0.4.1) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/venv/lib/python3.12/site-packages (from jinja2->torch<2.8.0,>=2.2.1->lerobot==0.4.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/venv/lib/python3.12/site-packages (from pandas->datasets<4.2.0,>=4.0.0->lerobot==0.4.1) (2025.2)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lerobot, transformers\n",
      "  Building editable for lerobot (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lerobot: filename=lerobot-0.4.1-0.editable-py3-none-any.whl size=15631 sha256=35cb40bb417d030257839a3c15c0886f61b20a4b5213bfa4eb5e2888f41b378f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j4p9fo5a/wheels/05/0a/0d/80a4c08845345c44fe1e5f70929884983b90d85f46a77f7601\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.53.3-py3-none-any.whl size=10827916 sha256=3615ad82b667702561074091a27d1ea4440996cb899e8c326614c4d4854ecd51\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j4p9fo5a/wheels/29/9c/af/9393d4342ca3b00939f0143419b1708e3d31ed8980de898431\n",
      "Successfully built lerobot transformers\n",
      "Installing collected packages: tokenizers, transformers, lerobot\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.1\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.1\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.57.3\n",
      "\u001b[2K    Uninstalling transformers-4.57.3:90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.3━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: lerobot\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: lerobot 0.4.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling lerobot-0.4.1:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled lerobot-0.4.1\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [lerobot]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [lerobot]\u001b[0m [lerobot]\n",
      "\u001b[1A\u001b[2KSuccessfully installed lerobot-0.4.1 tokenizers-0.21.4 transformers-4.53.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# For smolVLA\n",
    "!cd lerobot && pip install -e \".[smolvla]\"\n",
    "# For Pi\n",
    "!cd lerobot && pip install -e \".[pi]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you want to resume the training from last checkpoint, run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lerobot-train \\\n",
    "  --resume=true \\\n",
    "  --config_path=outputs/train/<job name>/checkpoints/last/pretrained_model/train_config.json \\\n",
    "  --steps=<new total steps>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005000\t010000\t015000\t020000\t025000\t030000\tlast\n"
     ]
    }
   ],
   "source": [
    "!ls outputs/train/so101_petcap_smolvla2/checkpoints/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
